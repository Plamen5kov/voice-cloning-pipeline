# Practical Hyperparameter Tuning & Deep Learning Guide

A focused, pragmatic guide based on essential deep learning concepts for real-world practice.

---

## 1. Hyperparameter Search Strategy

### Random Search vs Grid Search

**âœ… USE: Random Search**
```python
import numpy as np

# Random search - explores more values efficiently
for trial in range(25):  # Even 25 trials covers a lot
    learning_rate = 10 ** np.random.uniform(-4, -1)
    hidden_units = np.random.randint(64, 512)
    dropout = np.random.uniform(0.1, 0.5)
    
    # Train and evaluate
```

**âŒ AVOID: Grid Search**
- Exponentially expensive (k^n combinations)
- Wastes resources on less important hyperparameters
- Only tests few values per hyperparameter

**Why Random is Better:**
- Explores many more values for the most important hyperparameter
- You don't know in advance which hyperparameters matter most
- More efficient use of computational budget

---

## 2. Hyperparameter Priority (Limited Resources)

### Focus Your Effort Here:

**1st Priority - MUST TUNE:**
- **Learning Rate (Î±)** - Biggest impact on performance

**2nd Priority - Should tune if possible:**
- Number of hidden units
- Mini-batch size
- Î² (momentum) - if using momentum (default 0.9 is often good)

**3rd Priority - Nice to have:**
- Number of layers
- Learning rate decay

**4th Priority - Almost never tune:**
- Adam parameters (Î²â‚=0.9, Î²â‚‚=0.999, Îµ=10â»â¸)
- Weight initialization variance

### Practical Action Plan:

**With very limited resources (5-10 experiments):**
```python
# Focus ONLY on learning rate
for trial in range(10):
    lr = 10 ** np.random.uniform(-4, -1)
    train_model(lr=lr)
```

**With moderate resources (25-50 experiments):**
```python
# Add architecture exploration
for trial in range(50):
    lr = 10 ** np.random.uniform(-4, -1)
    hidden_units = np.random.choice([64, 128, 256, 512])
    batch_size = np.random.choice([32, 64, 128])
    train_model(lr=lr, hidden_units=hidden_units, batch_size=batch_size)
```

---

## 3. Log Scale for Learning Rate

### The Formula

For learning rate Î± âˆˆ [0.0001, 1.0] = [10â»â´, 10â°]:

```python
# Step 1: Sample exponent uniformly
r = np.random.uniform(-4, 0)

# Step 2: Compute Î±
alpha = 10**r

# Examples:
# r = -3.7  â†’  Î± â‰ˆ 0.0002
# r = -2.1  â†’  Î± â‰ˆ 0.0079
# r = -0.5  â†’  Î± â‰ˆ 0.316
```

### When to Use Log Scale

| Hyperparameter | Use Log Scale? | Formula |
|----------------|----------------|---------|
| Learning rate (Î±) | âœ… Yes | `r = np.random.uniform(-4, 0); Î± = 10**r` |
| L2 regularization (Î») | âœ… Yes | `r = np.random.uniform(-6, -2); Î» = 10**r` |
| Beta (Î² for momentum) | âœ… Yes (for 1-Î²) | `r = np.random.uniform(-3, -1); Î² = 1 - 10**r` |
| Dropout rate | âŒ No | `np.random.uniform(0.1, 0.5)` |
| Number of units | âŒ No | `np.random.randint(50, 500)` |
| Batch size | âŒ No | `np.random.choice([32, 64, 128, 256])` |

**Why?** Log scale ensures equal exploration across orders of magnitude.

---

## 4. Adam Optimizer Parameters

### Default Values (Use These!)

```python
optimizer = Adam(
    lr=0.001,        # â† ONLY tune this!
    beta1=0.9,       # â† Keep default
    beta2=0.999,     # â† Keep default
    epsilon=1e-8     # â† Keep default
)
```

**Key Insight:** Even with unlimited resources, Î²â‚, Î²â‚‚, and Îµ are rarely tuned. Defaults work excellently in practice.

**Priority:**
- Î²â‚, Î²â‚‚, Îµ are 4th tier (lowest priority)
- Save your limited resources for learning rate

---

## 5. Panda vs Caviar Approach

### Your Approach = Your Compute Budget

**Panda Approach ðŸ¼** (Limited compute)
- Train ONE model at a time
- Monitor daily, adjust hyperparameters based on performance
- Babysit the training process
- Common in: Academia, small teams, individual researchers

**Caviar Approach ðŸŸ** (Sufficient compute)
- Train MANY models in parallel
- Different hyperparameters simultaneously
- Pick best performer
- Common in: Industry, big tech companies

**Your choice depends entirely on available computational resources.**

---

## 6. When to Re-tune Hyperparameters

### âœ… Re-evaluate hyperparameters when:

1. **New data is added** - Data distribution may shift
2. **Computational resources change** - Different hardware affects optimal settings
3. **Every few months** - Regular practice
4. **Moving to new problem domain**

### Why?

Hyperparameters are NOT universal - they depend on:
- Dataset characteristics
- Hardware available
- Problem domain
- Time (data drifts)

**Action:** At minimum, re-tune learning rate when environment changes significantly.

---

## 7. Batch Normalization Essentials

### Key Parameters

**Î³ (gamma) and Î² (beta) are LEARNABLE PARAMETERS, not hyperparameters**

```python
# Batch norm algorithm
z_norm = (z - Î¼) / sqrt(ÏƒÂ² + Îµ)  # Normalize
z_tilde = Î³ * z_norm + Î²          # Scale and shift

# Where:
# Î³ controls variance
# Î² controls mean
# Both learned via backpropagation (like weights)
```

### Important Facts:

âœ… **Drop b[l] (bias term)** when using batch norm - it gets zeroed out  
âœ… **Keep W[l] (weights)** - still needed!  
âœ… **Îµ (epsilon = 10â»â¸)** prevents division by zero (numerical stability)  
âœ… **One Î³ and one Î² PER HIDDEN UNIT** - not one per layer  

### Batch Norm at Test Time

**âŒ WRONG:** Turn off batch norm  
**âœ… CORRECT:** Use running averages instead of batch statistics

```python
# Training: Use batch statistics
z_norm = (z - Î¼_batch) / sqrt(ÏƒÂ²_batch + Îµ)

# Test: Use running averages (exponentially weighted)
z_norm = (z - Î¼_running) / sqrt(ÏƒÂ²_running + Îµ)

# Still apply scale and shift
z_tilde = Î³ * z_norm + Î²
```

**Frameworks handle this automatically:**
```python
# PyTorch
model.train()  # Uses batch statistics
model.eval()   # Uses running averages

# TensorFlow/Keras
model.fit()     # training=True
model.predict() # training=False
```

---

## 8. Deep Learning Framework Selection

### Three Main Criteria:

1. **Ease of programming** (development speed)
2. **Running speed** (training and inference)
3. **Truly open source** (governance, licensing)

### NOT Selection Criteria:

âŒ Must use Python exclusively  
âŒ Must run only on cloud  
âŒ Must be implemented in C  

### Practical Recommendations:

- **Learning:** PyTorch or Keras
- **Production:** PyTorch or TensorFlow (both mature)
- **Research:** PyTorch or JAX
- **Mobile/Edge:** TensorFlow Lite

---

## 9. Quick Reference: Good Defaults

### Starting Point for Most Problems:

```python
# Optimizer
optimizer = 'Adam'
learning_rate = 0.001  # Start here, then tune

# Architecture
hidden_units = 128      # Try: 64, 128, 256, 512
batch_size = 64         # Try: 32, 64, 128
dropout = 0.2           # Try: 0.2-0.5 if overfitting

# Adam parameters (don't tune)
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8

# Training
epochs = 50  # With early stopping
```

### Diagnostic Quick Guide:

| Symptom | Diagnosis | Action |
|---------|-----------|--------|
| Cost increases/explodes | Learning rate too high | Reduce by 10x |
| Cost decreases very slowly | Learning rate too low | Increase by 2-5x |
| Cost decreases smoothly | Just right! âœ“ | Keep it |
| Good train, poor validation | Overfitting | Add dropout/regularization |

---

## 10. Practical Workflow

### Step-by-Step Process:

1. **Start with good defaults** (see section 9)
2. **Train baseline model** - establish performance floor
3. **Tune learning rate** - Random search, 10-25 experiments
4. **If resources allow:** Explore architecture (units, layers, batch size)
5. **If overfitting:** Add regularization (dropout, L2)
6. **Re-evaluate** when data/resources change

### Coarse-to-Fine Strategy:

```
Step 1: Sample broadly
        [-----------------------------]
        Find best region: [-----***-----]

Step 2: Zoom in on best region
        [***]
        Sample more densely

Step 3: Refine until satisfied
```

---

## 11. Common Pitfalls to Avoid

âŒ Spending too much time on low-priority hyperparameters  
âŒ Using grid search with many hyperparameters  
âŒ Using uniform sampling for learning rate  
âŒ Thinking optimal hyperparameters transfer across datasets  
âŒ Turning off batch norm at test time  
âŒ Treating Î³ and Î² as hyperparameters to tune  
âŒ Tuning Adam's Î²â‚, Î²â‚‚, Îµ when resources are limited  

---

## 12. Essential Code Snippets

### Random Hyperparameter Search

```python
import numpy as np

def sample_hyperparameters():
    """Sample hyperparameters using appropriate scales."""
    
    # LOG SCALE (use 10**r)
    r_lr = np.random.uniform(-4, -1)
    learning_rate = 10**r_lr  # [0.0001, 0.1]
    
    r_lambda = np.random.uniform(-6, -2)
    l2_lambda = 10**r_lambda  # [0.000001, 0.01]
    
    # LINEAR SCALE (use uniform/choice)
    dropout = np.random.uniform(0.1, 0.5)
    hidden_units = np.random.choice([64, 128, 256, 512])
    batch_size = np.random.choice([32, 64, 128])
    
    return {
        'learning_rate': learning_rate,
        'l2_lambda': l2_lambda,
        'dropout': dropout,
        'hidden_units': hidden_units,
        'batch_size': batch_size
    }

# Run search
best_score = 0
best_params = None

for trial in range(25):
    hp = sample_hyperparameters()
    score = train_and_evaluate(hp)
    
    if score > best_score:
        best_score = score
        best_params = hp
        print(f"New best: {score:.4f} with lr={hp['learning_rate']:.6f}")
```

### Batch Normalization Implementation

```python
def batch_norm_forward(z, gamma, beta, epsilon=1e-8):
    """Forward pass with batch normalization."""
    # Compute statistics
    mu = np.mean(z, axis=1, keepdims=True)
    sigma_sq = np.var(z, axis=1, keepdims=True)
    
    # Normalize
    z_norm = (z - mu) / np.sqrt(sigma_sq + epsilon)
    
    # Scale and shift
    z_tilde = gamma * z_norm + beta
    
    return z_tilde, (z_norm, mu, sigma_sq)
```

---

## Summary: Focus Areas for Practice

### ðŸŽ¯ High Priority (Master These):

1. **Random search implementation** for hyperparameters
2. **Log scale sampling** for learning rate (10**r formula)
3. **Learning rate tuning** as first priority
4. **When to re-tune** hyperparameters
5. **Batch norm at test time** (running averages)

### ðŸ“š Medium Priority (Understand Well):

6. Panda vs Caviar approaches
7. Adam optimizer defaults
8. Batch norm parameters (Î³, Î² as learnable, not hyperparams)
9. Framework selection criteria

### ðŸ’¡ Key Mindset:

- **Focus on learning rate** when resources are limited
- **Use random search**, not grid search
- **Use log scale** for hyperparameters spanning orders of magnitude
- **Defaults often work** for Adam parameters
- **Re-tune when environment changes**

---

*Created: February 14, 2026*
*Based on: Deep Learning Specialization concepts and practical experience*
