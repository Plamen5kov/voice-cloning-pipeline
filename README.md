# Voice Cloning Pipeline ğŸ™ï¸

A hands-on learning path for building a modern voice cloning pipeline using deep learning, TTS systems, and related open-source tools. This project is designed for learning, experimentation, and developing AI engineering skills through practical implementation.

---

## ğŸ“š Overview of Learning Materials

This repository contains a **structured learning path** with 12+ modules covering everything from Python basics to deploying production ML systems:

**Core Learning Areas:**
1. **Python Programming** (01_python_programming)
2. **Deep Learning Basics** (02_deep_learning_basics) â† Start here for ML fundamentals
3. **TTS Systems** (03_tts_systems)
4. **Speech/Audio Processing** (04_speech_audio_processing)
5. **Natural Language Processing** (05_nlp)
6. **Hugging Face Transformers** (06_hf_transformers)
7. **Data Preparation** (07_data_preparation)
8. **Model Training & Fine-tuning** (08_model_training_finetuning)
9. **Generative AI** (09_generative_ai)
10. **MLOps** (10_mlops)
11. **Cloud Platforms** (11_cloud_platforms)
12. **Project Building** (12_project_building)
13. **Capstone: Voice Replication Pipeline** (capstone_voice_replication_pipeline)

### Repository Structure
```
voice-cloning-pipeline/
â”œâ”€â”€ 00_env_setup/              # Initial environment setup
â”œâ”€â”€ 01_python_programming/     # Python basics & utilities
â”œâ”€â”€ 02_deep_learning_basics/   # PyTorch, neural networks, training loops â­
â”‚   â”œâ”€â”€ README.md              # Module overview
â”‚   â”œâ”€â”€ LEARNING_GUIDE.md      # Educational approach
â”‚   â”œâ”€â”€ ML_CONCEPTS_EXPLAINED.md  # Deep dive into concepts (30KB)
â”‚   â”œâ”€â”€ dl_utils.py            # Reusable utilities
â”‚   â””â”€â”€ 01-05 Python scripts   # Hands-on exercises
â”œâ”€â”€ 03_tts_systems/            # Text-to-speech implementation
â”œâ”€â”€ 04_speech_audio_processing/
â”œâ”€â”€ 05_nlp/
â”œâ”€â”€ 06_hf_transformers/
â”œâ”€â”€ 07_data_preparation/
â”‚   â””â”€â”€ data/                  # Datasets (not in git - see setup below)
â”œâ”€â”€ 08_model_training_finetuning/
â”œâ”€â”€ 09_generative_ai/
â”œâ”€â”€ 10_mlops/
â”œâ”€â”€ 11_cloud_platforms/
â”œâ”€â”€ 12_project_building/
â””â”€â”€ capstone_voice_replication_pipeline/
```

### Learning Resources by Type

The `02_deep_learning_basics` folder contains **three types of resources** that work together:

1. **ğŸ“– Conceptual Guides** - Theory and explanations
2. **ğŸ’» Executable Scripts** - Hands-on practice
3. **ğŸ“Š Generated Outputs** - Results from running scripts

---

## ğŸ¯ The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VOICE CLONING PIPELINE                        â”‚
â”‚                   (Your Ultimate Goal)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²
                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ TTS SYSTEMS   â”‚                   â”‚   NLP & ML   â”‚
    â”‚ (Folder 03)   â”‚                   â”‚ (Folders 05-12)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ DEEP LEARNING     â”‚
                    â”‚    BASICS         â”‚
                    â”‚ (Folder 02) â† YOU ARE HERE
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ PYTHON & PYTORCH  â”‚
                    â”‚ (Folder 01)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**You are currently building the foundation** - deep learning basics that everything else builds upon.

---

## ğŸ“‚ Document Structure & Reading Order

### ğŸ“ Start Here First

#### 1. **[README.md](02_deep_learning_basics/README.md)** - Your Starting Point
**Read first** - Gives context for what this folder is about

**What it covers**:
- Overview of the exercises
- How this fits into the larger pipeline
- Quick reference

**Time**: 2 minutes

---

### ğŸ“– Conceptual Understanding (Read These)

#### 2. **[LEARNING_GUIDE.md](02_deep_learning_basics/LEARNING_GUIDE.md)** - Educational Overview  
**Read second** - Explains what makes these scripts educational

**What it covers**:
- Overview of all scripts and what they teach
- Key learning points from each script
- Expected results
- How the pieces fit together
- What to focus on

**When to read**: Before diving into the scripts

**Time**: 15 minutes

**Why it matters**: Sets context so you understand the *purpose* behind each exercise

---

#### 3. **[ML_CONCEPTS_EXPLAINED.md](02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md)** - Deep Dive Reference â­
**Read third (and keep as reference)** - Comprehensive concept explanations

**What it covers** (14 major topics):
1. Gradients
2. Backpropagation
3. Loss Functions
4. Optimizers
5. Activation Functions
6. Overfitting & Underfitting
7. Regularization
8. Normalization
9. Learning Rate
10. Epochs, Batches & Iterations
11. Forward & Backward Pass
12. Model Capacity
13. Generalization
14. Hyperparameters

**When to read**: 
- Initial read-through (skim all topics)
- Deep dive when you encounter concepts in scripts
- Reference when confused

**Time**: 1-2 hours for full read, 5-10 minutes per concept

**How to use**:
```
Encounter "gradient" in script
    â†“
Look up in ML_CONCEPTS_EXPLAINED.md
    â†“
Read explanation with examples
    â†“
Return to script with understanding
```

**Why it matters**: This is your encyclopedia - every time you see unfamiliar terminology, look it up here!

---

### ğŸ’» Hands-On Practice (Run These in Order)

#### 4. **[dl_utils.py](02_deep_learning_basics/dl_utils.py)** - Utility Functions Library
**Don't run directly** - Helper functions used by all other scripts

**What it contains**:
- `get_device()` - GPU/CPU detection
- `load_mnist_data()` - Data loading with normalization
- `evaluate_model()` - Accuracy calculation
- `plot_training_history()` - Visualization
- More utilities...

**Why it exists**: Eliminates code duplication, makes scripts cleaner

**Educational value**: 
- Read the docstrings (they explain WHY each choice matters)
- Understand normalization, batching, evaluation

**When to read**: After running first script, to understand what's happening under the hood

---

#### 5. **[01_hello_pytorch.py](02_deep_learning_basics/01_hello_pytorch.py)** - PyTorch Basics
**Run first** - Get comfortable with PyTorch

**What you'll learn**:
- Creating tensors
- Basic operations
- GPU acceleration
- Why PyTorch for deep learning

**Prerequisites**: None!

**Run it**:
```bash
cd 02_deep_learning_basics
python 01_hello_pytorch.py
```

**Expected output**: Tensor operations, GPU info

**Time**: 5 minutes

**Next step**: Once comfortable, move to loading data

---

#### 6. **[02_load_mnist.py](02_deep_learning_basics/02_load_mnist.py)** - Data Exploration
**Run second** - Understand your data

**What you'll learn**:
- What MNIST dataset is
- Why data exploration matters
- Class distribution
- Data visualization

**Prerequisites**: Understanding of tensors (from script 01)

**Concepts used** (from ML_CONCEPTS_EXPLAINED.md):
- Normalization
- Data preprocessing

**Run it**:
```bash
cd 02_deep_learning_basics
python 02_load_mnist.py
```

**Output files**: `mnist_samples.png`

**What to observe**:
- Are digits clear or ambiguous?
- Class balance (each digit appears ~6000 times)
- Image quality

**Time**: 5 minutes

**Next step**: Now that you know the data, train a model!

---

#### 7. **[03_train_mnist.py](02_deep_learning_basics/03_train_mnist.py)** - Training Loop â­ MOST IMPORTANT
**Run third** - The heart of deep learning

**What you'll learn**:
- Neural network architecture
- The training loop (forward, backward, update)
- How backpropagation works
- Hyperparameter choices

**Prerequisites**: 
- Data understanding (script 02)
- Concepts from ML_CONCEPTS_EXPLAINED.md

**Key concepts used**:
- **Gradients** - Core of learning
- **Backpropagation** - How gradients are computed
- **Loss Functions** - CrossEntropyLoss
- **Optimizers** - Adam
- **Activation Functions** - ReLU
- **Epochs & Batches** - Training organization

**Run it**:
```bash
cd 02_deep_learning_basics
python 03_train_mnist.py
```

**Output files**: 
- `mnist_model.pth` - Saved model
- `training_history.png` - Training curves

**What to observe**:
- Training accuracy increasing
- Validation accuracy tracking training
- Small gap = good generalization
- ~97-98% final accuracy

**Time**: 30 seconds on GPU, 3 minutes on CPU

**Reading the code**:
1. Start with the `SimpleNN` class - read all comments
2. Read `train_epoch()` - this is THE CORE of deep learning
3. Read the hyperparameter section
4. Run it and watch the magic!

**âš ï¸ CRITICAL**: This script has the most educational comments. Read every comment carefully!

**Next step**: Verify the model persists

---

#### 8. **[04_load_model.py](02_deep_learning_basics/04_load_model.py)** - Model Persistence
**Run fourth** - Save and reuse models

**What you'll learn**:
- Why model saving matters
- How to load and use trained models
- Inference on new data
- Model state management

**Prerequisites**: Trained model from script 03

**Concepts used**:
- Model persistence
- Evaluation mode
- Inference

**Run it**:
```bash
cd 02_deep_learning_basics
python 04_load_model.py
```

**Expected output**: 
- Loaded model confirmation
- Test accuracy (should match script 03)
- Sample predictions with confidence scores

**Time**: 10 seconds

**What to observe**:
- Same accuracy as training (model saved correctly)
- High confidence on correct predictions
- How to interpret outputs

**Next step**: Experiment with variations

---

#### 9. **[05_experiment_architectures.py](02_deep_learning_basics/05_experiment_architectures.py)** - Compare Architectures
**Run fifth** - Understand architecture tradeoffs

**What you'll learn**:
- How depth affects performance
- ReLU vs Tanh activation
- Parameter count implications
- Architecture design choices

**Prerequisites**: Understanding from scripts 01-04

**Concepts used**:
- **Model Capacity** - More parameters vs overfitting
- **Activation Functions** - ReLU vs Tanh
- **Architecture Design** - Depth vs width

**Run it**:
```bash
cd 02_deep_learning_basics
python 05_experiment_architectures.py
```

**Output files**: `architecture_comparison.png`

**Expected results**:
- 3-Layer ReLU: ~97.9% (best, but 2x parameters)
- 2-Layer ReLU: ~97.5% (good efficiency)
- 2-Layer Tanh: ~97.2% (slightly worse)

**Time**: 2-3 minutes

**What to observe**:
- Deeper isn't always better (for simple problems)
- ReLU generally outperforms Tanh
- Tradeoff between capacity and overfitting

**Analysis**:
- Look at the comparison plot
- Which architecture is best for MNIST?
- When would you choose each?

---

## ğŸ“ Recommended Learning Path
02_deep_learning_basics/README.md (2 min)
2. Read: 02_deep_learning_basics/LEARNING_GUIDE.md (15 min)
3. Skim: 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md (30 min)
   - Focus on: Gradients, Loss Functions, Optimizers
4. Run: 02_deep_learning_basics/01_hello_pytorch.py
5. Run: 02_deep_learning_basics/ ML_CONCEPTS_EXPLAINED.md (30 min)
   - Focus on: Gradients, Loss Functions, Optimizers
4. Run: 01_hello_pytorch.py
5. Run: 02_load_mnist.py
```

**Goal**: Understand the context and get comfortable with PyTorch

---
02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md sections:
   - Gradients
   - Backpropagation
   - Forward & Backward Pass
   - Learning Rate
   
2. Run: 02_deep_learning_basics/03_train_mnist.py
   - Read EVERY comment in the code
   - Watch training progress
   - Understand each step

3. Read: 02_deep_learning_basics/dl_utils.py docstrings
   - Understand what each utility does
   
4. Run: 02_deep_learning_basics/ dl_utils.py docstrings
   - Understand what each utility does
   
4. Run: 04_load_model.py
```

**Goal**: Deeply understand the training loop - this is the core of deep learning!

---
2_deep_learning_basics/05_experiment_architectures.py
   - Compare different architectures
   
2. Experiment yourself:
   - Modify 02_deep_learning_basics/03_train_mnist.py
   - Try different learning rates
   - Change batch sizes
   - Add more layers
   - Watch what happens!
   
3. Reference: 02_deep_learning_basics/layers
   - Watch what happens!
   
3. Reference: ML_CONCEPTS_EXPLAINED.md
   - Look up concepts as needed
   - Deepen understanding
```

**Goal**: Build intuition through experimentation

---
02_deep_learning_basics/
### Phase 4: Mastery (Ongoing)
```
1. Revisit ML_CONCEPTS_EXPLAINED.md
   - Deep dive into advanced topics
   - Overfitting & Regularization
   - Hyperparameters
   - Generalization

2. Try challenges:
   - Can you get >98% accuracy?
   - Can you do it with fewer parameters?
   - Can you add data augmentation?
   
3. Move to next folder (04_speech_audio_processing)
```

**Goal**: Solidify understanding before moving on

---

## ğŸ”„ How Documents Work Together

### Workflow Example
```
1. See code in 02_deep_learning_basics/03_train_mnist.py:
   loss.backward()
   
2. Think: "What does backward() do?"

3. Check 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md:
   â†’ Backpropagation section
   â†’ Gradients section
   
4. Read: "Computes gradients using chain rule..."

5. Return to code with understanding

6. See it in action when running script

7. Check 02_deep_learning_basics/LEARNING_GUIDE.md for big picture
```

### Reference Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  02_deep_learning_basics/README.md  â”‚ â†’ Where am I? What's this folder?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 02_deep_learning_basics/            â”‚ â†’ What should I learn? In what order?
â”‚ LEARNING_GUIDE.md                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 02_deep_learning_basics/            â”‚ â†’ Hands-on practice
â”‚ Python Scripts (01-05)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ (when confused)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 02_deep_learning_basics/            â”‚ â†’ Deep explanation of concepts
â”‚ ML_CONCEPTS_EXPLAINED.md            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 02_deep_learning_basics/            â”‚ â†’ Why these implementations?
â”‚ dl_utils.py (docstrings)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š What Each Script Produces

| Script | Output Files | What to Look For |
|--------|-------------|------------------|
| 01_hello_pytorch.py | Console output | Tensor operations, GPU detection |
| 02_load_mnist.py | mnist_samples.png | Data quality, class distribution |
| 03_train_mnist.py | mnist_model.pth, training_history.png | Training curves, convergence |
| 04_load_model.py | Console output | Same accuracy, correct predictions |
| 05_experiment_architectures.py | architecture_comparison.png | Performance comparison |

---

## ğŸ¯ Learning Objectives by Document

### README.md
- âœ… Understand what this folder contains
- âœ… Know the overall structure

### LEARNING_GUIDE.md
- âœ… Understand the learning path
- âœ… Know what each script teaches
- âœ… See expected results

### ML_CONCEPTS_EXPLAINED.md
- âœ… Deeply understand gradients and backpropagation
- âœ… Know when to use different loss functions
- âœ… Understand optimizer choices
- âœ… Grasp overfitting vs underfitting
- âœ… Master hyperparameter tuning

### Python Scripts (01-05)
- âœ… Hands-on experience with PyTorch
- âœ… Build and train neural networks
- âœ… Understand the training loop
- âœ… Evaluate and save models
- âœ… Experiment with architectures

### dl_utils.py
- âœ… Understand why normalization matters
- âœ… Learn batch processing
- âœ… Grasp evaluation best practices

---

## ğŸš€ After Mastering This Folder

### You Should Understand:
- âœ… How neural networks learn (gradients, backprop)
- âœ… The training loop (forward, backward, update)
- âœ… How to evaluate models
- âœ… Overfitting vs underfitting
- âœ… Basic hyperparameter tuning

### You'll Be Ready For:
- ğŸ“ **04_speech_audio_processing** - Apply DL to audio
- ğŸ“ **05_nlp** - Natural language processing
- ğŸ“ **06_hf_transformers** - Modern architectures
- ğŸ“ **07_data_preparation** - Real-world data handling

### Skills Gained:
1. **Practical**: Can train simple neural networks
2. **Conceptual**: Understand core DL principles
3. **Debugging**: Can diagnose training issues
4. **Experimentation**: Can modify and test architectures

---

## ğŸ“ Study Tips

### Active Learning
```
âŒ Don't: Just read the code
âœ… Do: Run it, modify it, break it, fix it

âŒ Don't: Skip the conceptual docs
âœ… Do: Reference them constantly

âŒ Don't: Memorize formulas
âœ… Do: Understand the intuition
```

### When Stuck
```
1. Check 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md for the concept
2. Re-read the inline comments in the script
3. Experiment: Change one thing, see what happens
4. Check 02_deep_learning_basics/LEARNING_GUIDE.md for context
5. Run the code and observe outputs
```

### Retention Strategy
```
Day 1: Learn concept (read + run)
Day 2: Revisit and experiment
Day 7: Quick review of key concepts
Day 30: Apply to new problem
```

---

## ğŸ“ Concept Dependency Tree

Understanding how concepts build on each other:

```
Tensors (01)
    â†“
Data Loading (02)
    â†“
Loss Functions â†â”€â”€â”€â”€â”€â”
    â†“                â”‚
Gradients â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â†“                â”‚
Backpropagation      â”‚
    â†“                â”‚
Optimizers           â”‚
    â†“                â”‚
Training Loop (03) â”€â”€â”˜
    â†“
Evaluation
    â†“
Persistence (04)
    â†“
Architecture Design (05)
```

**Key insight**: Each concept builds on previous ones. If confused, go back to prerequisites!

---

## ğŸ” Quick Reference Cheat Sheet

### When you see this term â†’ Look here:
02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Gradients |
| loss.backward() | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Backpropagation |
| CrossEntropyLoss | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Loss Functions |
| Adam optimizer | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Optimizers |
| ReLU | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Activation Functions |
| Overfitting | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Overfitting & Underfitting |
| Dropout | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Regularization |
| Normalize | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Normalization |
| Learning rate | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Learning Rate |
| Epoch | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Epochs, Batches & Iterations |
| model.eval() | 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md | Forward & Backward Pass |
| Why 128 neurons? | 02_deep_learning_basics/03_train_mnist.py | SimpleNN class comments |
| Why batch_size=64? | 02_deep_learning_basics/dl_utils.py | load_mnist_data() docstring |
| Why normalize? | 02_deep_learning_basics/_CONCEPTS_EXPLAINED.md | Forward & Backward Pass |
| Why 128 neurons? | 03_train_mnist.py | SimpleNN class comments |
| Why batch_size=64? | dl_utils.py | load_mnist_data() docstring |
| Why normalize? | dl_utils.py | load_mnist_data() docstring |

---

## â“ Common Questions - Quick Answers

**Q: "Why do we call optimizer.zero_grad()?"**  
A: 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md â†’ Gradients section

**Q: "What's the difference between train and test accuracy?"**  
A: 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md â†’ Generalization section

**Q: "Should I use SGD or Adam?"**  
A: 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md â†’ Optimizers section (comparison table)

**Q: "How do I know if I'm overfitting?"**  
A: 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md â†’ Overfitting & Underfitting section

**Q: "What learning rate should I use?"**  
A: 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md â†’ Learning Rate section (typical values)

**Q: "Why 10 epochs?"**  
A: 02_deep_learning_basics/03_train_mnist.py â†’ Hyperparameters section comments

**Q: "What is backpropagation really?"**  
A: 02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md â†’ Backpropagation section (+ mountain analogy in Gradients)

---

## ğŸ¯ Success Metrics

You've mastered this material when you can:

- [ ] Explain what a gradient is to someone else
- [ ] Write a training loop from scratch
- [ ] Diagnose overfitting from a training plot
- [ ] Choose appropriate hyperparameters
- [ ] Explain why we use ReLU over Tanh
- [ ] Understand every line in 02_deep_learning_basics/03_train_mnist.py
- [ ] Modify architectures and predict the effect
- [ ] Know when to normalize data and why

---

## ğŸ—ºï¸ Your Journey

```
Current Location: 02_deep_learning_basics
Status: Building Foundation

You are here:
    â†“
[âœ“] 00_env_setup
[âœ“] 01_python_programming  
[â†’] 02_deep_learning_basics â† CURRENT
[ ] 03_tts_systems (completed earlier)
[ ] 04_speech_audio_processing
[ ] 05_nlp
[ ] 06_hf_transformers
[ ] 07_data_preparation
[ ] 08_model_training_finetuning
[ ] 09_generative_ai
[ ] 10_mlops
[ ] 11_cloud_platforms
[ ] 12_project_building
[ ] capstone_voice_replication_pipeline
```

**Next Destination**: Speech & Audio Processing (applying DL to audio)

---

## ğŸ“Œ Summary

### Three Pillars of Deep Learning Basics:

1. **Theory** (02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md)
   - Comprehensive reference
   - Look up concepts as needed
   - Build deep understanding

2. **Practice** (02_deep_learning_basics/Python Scripts 01-05)
   - Hands-on experience
   - Learn by doing
   - Experiment and break things

3. **Guidance** (02_deep_learning_basics/LEARNING_GUIDE.md + this ROADMAP)
   - Navigate the materials
   - Understand connections
   - Stay on track

### Golden Rule:
**Don't just read code. Run it. Modify it. Break it. Understand it.**

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- PyTorch 2.0+
- CUDA-capable GPU (optional but recommended)
- Basic command-line knowledge

### Quick Start
1. **Clone this repository**
   ```bash
   git clone https://github.com/yourusername/voice-cloning-pipeline.git
   cd voice-cloning-pipeline
   ```

2. **Start with environment setup**
   ```bash
   cd 00_env_setup
   # Follow instructions in README.md
   ```

3. **Begin learning path**
   ```bash
   cd ../02_deep_learning_basics
   python 01_hello_pytorch.py
   ```

### Data Setup

**Note:** The `data/` folders are excluded from git via `.gitignore` to keep the repository lightweight. You must manually download or generate required datasets.

#### Downloading LibriTTS Sample Data (for TTS training)

1. Visit [LibriTTS on OpenSLR](https://www.openslr.org/60/) or [LibriTTS on Hugging Face](https://huggingface.co/datasets/lj1995/LibriTTS)
2. Download desired subset (e.g., `dev-clean`)
3. Extract to `07_data_preparation/data/libritts_sample/LibriTTS/`
4. Your structure should look like:
   ```
   07_data_preparation/
   â””â”€â”€ data/
       â””â”€â”€ libritts_sample/
           â””â”€â”€ LibriTTS/
               â””â”€â”€ dev-clean/
   ```

**Important:** Do not add large datasets or audio files to git!

---

## ğŸ“– Capstone Project: Custom Voice Replication Pipeline

**Goal:** Build an end-to-end pipeline that processes text and generates speech in your own voice.

**Key Steps:**
1. **Data Collection** - Record voice samples (10 short + 10 long)
2. **Preprocessing** - Trim, normalize, convert to WAV 24kHz mono
3. **Feature Extraction** - Extract MFCCs and spectrograms with librosa
4. **Model Training** - Train TTS model (Tacotron/FastSpeech/Coqui TTS)
5. **Text Processing** - Use NLP to clean and segment input
6. **Inference** - Generate speech from new text
7. **Deployment** - Build REST API (Flask/FastAPI)
8. **Evaluation** - Test quality and iterate

**Outcome:** A modular, working system that can read any text in your own voice, with all core AI skills practiced through manageable tasks.

---

## ğŸ¯ AI Spheres Covered

This project touches multiple areas of AI:

| AI Sphere | Relevance to Voice Cloning | Technologies |
|-----------|---------------------------|--------------|
| **Deep Learning** â­ | Neural networks for speech synthesis | PyTorch, TensorFlow |
| **Speech & Audio Processing** â­ | TTS, voice conversion, audio enhancement | Coqui TTS, Bark, librosa |
| **NLP** â­ | Text parsing, summarization, dialogue detection | Hugging Face, spaCy |
| **Generative AI** â­ | Synthesizing unique voices, creating content | GPT, Bark, Stable Diffusion |
| **MLOps** | Model deployment, monitoring, automation | MLflow, Docker, FastAPI |
| **Cloud Platforms** | Scalable deployment | AWS, GCP, Azure |
| **Supervised Learning** | Audio classification, quality control | Scikit-learn |
| **Multi-modal AI** | Syncing narration with visuals | CLIP, Transformers |

â­ = Core focus areas for voice cloning

---

## ğŸ“ Learning Approach

This repository uses a **task-based learning approach** with:
- âœ… Hands-on Python scripts with extensive educational comments
- âœ… Comprehensive concept explanations ([ML_CONCEPTS_EXPLAINED.md](02_deep_learning_basics/ML_CONCEPTS_EXPLAINED.md))
- âœ… Clear learning objectives and expected outcomes
- âœ… Incremental complexity (basic â†’ advanced)
- âœ… Real-world project structure

---

## ğŸ“Š Deep Learning Basics - Detailed Guide

The following sections provide comprehensive navigation for the deep learning module, which forms the foundation for all subsequent work.

---

Now you know exactly where you are and where you're going. Happy learning! ğŸš€
