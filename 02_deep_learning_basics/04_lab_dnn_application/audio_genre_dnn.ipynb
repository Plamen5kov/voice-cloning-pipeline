{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6886414c",
   "metadata": {},
   "source": [
    "# Deep Neural Network for Voice Gender Classification: Application\n",
    "\n",
    "Welcome to the advanced audio classification lab! Building on what you learned in Lab 1 (binary speech vs music classification), you'll now use deep neural networks to classify voice recordings as male or female.\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a deep L-layer neural network for binary voice classification\n",
    "- Apply deep learning to real-world voice analysis tasks\n",
    "- Compare shallow vs deep network performance on audio data\n",
    "- Understand why deeper networks work better for extracting voice features\n",
    "- Work with real audio data from TTS systems\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb23ef0",
   "metadata": {},
   "source": [
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb6b09",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Load and Process the Dataset](#2)\n",
    "- [3 - Model Architecture](#3)\n",
    "    - [3.1 - 2-layer Neural Network](#3-1)\n",
    "    - [3.2 - L-layer Deep Neural Network](#3-2)\n",
    "    - [3.3 - General Methodology](#3-3)\n",
    "- [4 - Two-layer Neural Network](#4)\n",
    "    - [Exercise 1 - two_layer_model](#ex-1)\n",
    "    - [4.1 - Train the model](#4-1)\n",
    "- [5 - L-layer Neural Network](#5)\n",
    "    - [Exercise 2 - L_layer_model](#ex-2)\n",
    "    - [5.1 - Train the model](#5-1)\n",
    "- [6 - Results Analysis](#6)\n",
    "- [7 - Test with your own audio (optional/ungraded exercise)](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3d842",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages\n",
    "\n",
    "Begin by importing all the packages you'll need during this assignment. \n",
    "\n",
    "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- [librosa](https://librosa.org/) is a library for audio analysis.\n",
    "- `dnn_app_utils_v3` provides the functions implemented in the \"Building your Deep Neural Network: Step by Step\" assignment to this notebook.\n",
    "- `audio_utils` provides functions to load and process audio data.\n",
    "- `np.random.seed(1)` is used to keep all the random function calls consistent. It helps grade your work - so please don't change it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b01966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from dnn_app_utils_v3 import *\n",
    "from audio_utils import *\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa54f30",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Load and Process the Dataset\n",
    "\n",
    "You'll be using real voice recordings for binary gender classification. In Lab 1, you classified audio as speech or music. Now you'll classify voice recordings as male or female:\n",
    "\n",
    "- **0: Male** - Male voice recordings\n",
    "- **1: Female** - Female voice recordings\n",
    "\n",
    "**Problem Statement**: You are given a dataset containing:\n",
    "    - a training set of `m_train` voice clips labeled by gender (0 or 1)\n",
    "    - a test set of `m_test` voice clips labeled by gender\n",
    "    - each audio clip is converted to a mel-spectrogram of shape (n_mels, time_steps)\n",
    "\n",
    "**Data Source**: We'll use real voice samples from TTS systems (like the speaker samples from Lab 3).\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c72f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_audio_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14912bc3",
   "metadata": {},
   "source": [
    "The following code will show you a spectrogram in the dataset. Feel free to change the index and re-run the cell multiple times to check out other spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d36d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a spectrogram\n",
    "index = 200\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(train_x_orig[index], x_axis='time', y_axis='mel', sr=22050)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(f\"Mel-Spectrogram - Gender: {classes[int(train_y[0,index])]}\")\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a '\" + classes[int(train_y[0,index])] +  \"' voice.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4399bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "n_mels = train_x_orig.shape[1]\n",
    "time_steps = train_x_orig.shape[2]\n",
    "m_test = test_x_orig.shape[0]\n",
    "num_classes = len(classes)\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Number of classes: \" + str(num_classes))\n",
    "print (\"Each spectrogram is of size: (\" + str(n_mels) + \", \" + str(time_steps) + \")\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))\n",
    "print (\"\\nGender classes: \" + str(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e29a8",
   "metadata": {},
   "source": [
    "As usual, you reshape and standardize the spectrograms before feeding them to the network. The code is given in the cell below.\n",
    "\n",
    "**Note**: Just like images are flattened from (height, width, channels) to vectors, spectrograms are flattened from (n_mels, time_steps) to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0326bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccfde0",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "The input size is the number of mel-frequency bins multiplied by the number of time steps in the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6884e",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Model Architecture\n",
    "\n",
    "Now that you're familiar with the dataset, it's time to build a deep neural network to classify audio by genre!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cb75f",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - 2-layer Neural Network\n",
    "\n",
    "You're going to build two different models:\n",
    "\n",
    "- A 2-layer neural network\n",
    "- An L-layer deep neural network\n",
    "\n",
    "Then, you'll compare the performance of these models, and try out some different values for $L$. \n",
    "\n",
    "The 2-layer model architecture:\n",
    "\n",
    "**INPUT (Mel-Spectrogram) -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT (Gender Probability)**\n",
    "\n",
    "<u><b>Detailed Architecture</b></u>:\n",
    "- The input is a mel-spectrogram which is flattened to a vector of size $(n\\_mels \\times time\\_steps, 1)$\n",
    "- The vector is multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]}, input\\_size)$\n",
    "- Add a bias term and take its relu to get the hidden layer activations\n",
    "- Multiply by $W^{[2]}$ and add bias\n",
    "- Apply sigmoid to get probability of female voice (1) vs male voice (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ceeab9",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - L-layer Deep Neural Network\n",
    "\n",
    "For a deeper network:\n",
    "\n",
    "**[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID**\n",
    "\n",
    "<u><b>Detailed Architecture</b></u>:\n",
    "- The input is a mel-spectrogram flattened to a vector\n",
    "- The vector is multiplied by $W^{[1]}$ and then you add the intercept $b^{[1]}$\n",
    "- Take the relu activation\n",
    "- This process repeats for each $(W^{[l]}, b^{[l]})$ layer\n",
    "- Finally, apply sigmoid to get probability of female voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a808861",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - General Methodology\n",
    "\n",
    "As usual, you'll follow the Deep Learning methodology to build the model:\n",
    "\n",
    "1. Initialize parameters / Define hyperparameters\n",
    "2. Loop for num_iterations:\n",
    "    a. Forward propagation\n",
    "    b. Compute cost function\n",
    "    c. Backward propagation\n",
    "    d. Update parameters (using parameters, and grads from backprop) \n",
    "3. Use trained parameters to predict labels\n",
    "\n",
    "Now go ahead and implement those two models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d2964",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Two-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4832d",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - two_layer_model \n",
    "\n",
    "Use the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: *LINEAR -> RELU -> LINEAR -> SOFTMAX*. \n",
    "\n",
    "**Note**: For binary classification (male vs female), we use sigmoid in the output layer with a single output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = train_x.shape[0]  # input size (n_mels * time_steps)\n",
    "n_h = 20                 # hidden layer size\n",
    "n_y = 1                  # output size (1 for binary classification)\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "learning_rate = 0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (0=male, 1=female), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "        #(≈ 2 lines of code)\n",
    "        # A1, cache1 = ...\n",
    "        # A2, cache2 = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute cost\n",
    "        #(≈ 1 line of code)\n",
    "        # cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        # For sigmoid + binary cross-entropy\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation\n",
    "        #(≈ 2 lines of code)\n",
    "        # dA1, dW2, db2 = ...\n",
    "        # dA0, dW1, db1 = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Set grads\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters\n",
    "        #(approx. 1 line of code)\n",
    "        # parameters = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Retrieve parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    return parameters, costs\n",
    "\n",
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfa37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2, print_cost=False)\n",
    "\n",
    "print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "\n",
    "two_layer_model_test(two_layer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a84b5",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```\n",
    "Cost after first iteration: ~0.69 (around -log(0.5) for random binary initialization)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65eb64",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Train the model \n",
    "\n",
    "If your code passed the previous cell, run the cell below to train your parameters. \n",
    "\n",
    "- The cost should decrease on every iteration. \n",
    "- It may take up to 5 minutes to run 2500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ea317",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2000, print_cost=True)\n",
    "plot_costs(costs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211fbf5",
   "metadata": {},
   "source": [
    "Now, you can use the trained parameters to classify voice recordings from the dataset. To see your predictions on the training and test sets, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d432c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53bf63",
   "metadata": {},
   "source": [
    "**Note**: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called \"early stopping\" and is a way to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8e4a6",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - L-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623bf16",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - L_layer_model \n",
    "\n",
    "Use the helper functions you implemented previously to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a112e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [n_x, 25, 15, 10, n_y] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (0=male, 1=female), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SOFTMAX.\n",
    "        #(≈ 1 line of code)\n",
    "        # AL, caches = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute cost\n",
    "        #(≈ 1 line of code)\n",
    "        # cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "        # Backward propagation\n",
    "        #(≈ 1 line of code)\n",
    "        # grads = ...    \n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    " \n",
    "        # Update parameters\n",
    "        #(≈ 1 line of code)\n",
    "        # parameters = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1, print_cost = False)\n",
    "\n",
    "print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "\n",
    "L_layer_model_test(L_layer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a96891",
   "metadata": {},
   "source": [
    "<a name='5-1'></a>\n",
    "### 5.1 - Train the model \n",
    "\n",
    "If your code passed the previous cell, run the cell below to train your model as a 4-layer neural network. \n",
    "\n",
    "- The cost should decrease on every iteration. \n",
    "- It may take up to 5 minutes to run 2500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6227f",
   "metadata": {},
   "source": [
    "### Congrats! It seems that your 4-layer neural network has better performance than your 2-layer neural network on multi-genre audio classification!\n",
    "\n",
    "This demonstrates how deeper networks can learn more complex patterns in audio data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073132e",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "##  6 - Results Analysis\n",
    "\n",
    "First, take a look at some audio clips the L-layer model labeled incorrectly. This will show a few misclassified spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe20a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mislabeled_audio(classes, test_x, test_y, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb7e67",
   "metadata": {},
   "source": [
    "**A few types of audio the model tends to misclassify:** \n",
    "- Mixed audio (speech with background music)\n",
    "- Low-quality recordings with noise\n",
    "- Similar genres (e.g., ambient vs sound effects)\n",
    "- Very short or truncated clips\n",
    "- Unusual or atypical examples within a genre\n",
    "\n",
    "**Why deeper networks help:**\n",
    "- More layers can learn hierarchical audio features\n",
    "- Early layers detect low-level patterns (frequencies, rhythms)\n",
    "- Later layers combine these into high-level genre characteristics\n",
    "- Better capacity to separate complex, overlapping patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ee4e5",
   "metadata": {},
   "source": [
    "### Congratulations on finishing this assignment! \n",
    "\n",
    "You just built and trained a deep L-layer neural network for audio genre classification! \n",
    "\n",
    "You've seen how:\n",
    "- Deep networks outperform shallow ones for complex audio patterns\n",
    "- The same principles from image classification apply to audio spectrograms\n",
    "- Network depth is crucial for learning hierarchical representations\n",
    "\n",
    "If you'd like to test your model with your own audio, there's an optional ungraded exercise below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b70bd",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Test with your own audio (optional/ungraded exercise)\n",
    "\n",
    "From this point, if you so choose, you can use your own audio to test the output of your model. To do that follow these steps:\n",
    "\n",
    "1. Add your audio file to the \"data/\" folder\n",
    "2. Change the audio filename in the following code\n",
    "3. Run the code and check if the algorithm correctly classifies it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe474f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## START CODE HERE ##\n",
    "my_audio = \"my_voice_sample.wav\" # change this to the name of your audio file \n",
    "my_label_y = [1] # the true class of your audio (0=speech, 1=music, 2=ambient, 3=sound_effects, 4=mixed)\n",
    "## END CODE HERE ##\n",
    "\n",
    "# Load and process your audio\n",
    "audio_path = \"data/\" + my_audio\n",
    "spectrogram = load_and_process_audio(audio_path)\n",
    "\n",
    "# Visualize the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(spectrogram, x_axis='time', y_axis='mel', sr=22050)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Your Audio - Mel-Spectrogram')\n",
    "plt.show()\n",
    "\n",
    "# Flatten and normalize\n",
    "spec_flatten = spectrogram.reshape((1, -1)).T\n",
    "spec_normalized = spec_flatten / 255.\n",
    "\n",
    "# Predict\n",
    "my_predicted_genre = predict(spec_normalized, my_label_y, parameters)\n",
    "\n",
    "print (\"y = \" + str(np.squeeze(my_predicted_genre)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_genre))] +  \"\\\" audio clip.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321121f",
   "metadata": {},
   "source": [
    "**Comparison with Lab 1:**\n",
    "\n",
    "| Aspect | Lab 1 (Logistic Regression) | Lab 4 (Deep Neural Network) |\n",
    "|--------|------------------------------|------------------------------|\n",
    "| **Task** | Binary (Speech vs Music) | Multi-class (5 genres) |\n",
    "| **Model** | Single layer | L-layer (up to 4+ layers) |\n",
    "| **Activation** | Sigmoid only | ReLU + Softmax |\n",
    "| **Complexity** | Linear decision boundary | Non-linear hierarchical |\n",
    "| **Accuracy** | ~70-75% (binary) | ~75-80% (5-class) |\n",
    "| **Learning** | Simple patterns | Complex audio features |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
