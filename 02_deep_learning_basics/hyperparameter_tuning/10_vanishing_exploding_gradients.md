# Vanishing / Exploding Gradients

**Source:** DeepLearning.AI - Practical Aspects of Deep Learning  
**Duration:** 0:05 / 6:07

## Introduction

One of the problems of training neural networks, **especially very deep neural networks**, is vanishing and exploding gradients.

**What this means:**
- When training a very deep network
- Your derivatives (slopes) can get either:
  - **Very, very big** (exploding)
  - **Very, very small**, maybe even exponentially small (vanishing)
- This makes training **difficult**

In this video, we'll see:
1. What exploding and vanishing gradients really means
2. How careful random weight initialization can significantly reduce this problem

## Setting Up the Problem

### A Very Deep Neural Network

```
Input (x) â†’ [Layer 1] â†’ [Layer 2] â†’ [Layer 3] â†’ ... â†’ [Layer L] â†’ Å·
             W[1],b[1]   W[2],b[2]   W[3],b[3]         W[L],b[L]
```

For visualization, let's say we have **only 2 hidden units per layer** (though it could be more).

**Parameters:** W[1], W[2], W[3], ..., W[L] and corresponding biases

### Simplifying Assumptions

To make the math clearer, let's assume:

1. **Linear activation function:** g(z) = z
2. **No bias:** b[l] = 0 for all layers

These assumptions allow us to see the core problem more clearly.

## Mathematical Analysis

### Forward Propagation with Simplified Network

With our assumptions, the output Å· will be:

```
Å· = W[L] Â· W[L-1] Â· W[L-2] Â· ... Â· W[3] Â· W[2] Â· W[1] Â· x
```

**Why?** Let's verify:

```
Layer 1:
  z[1] = W[1] Â· x + b[1] = W[1] Â· x     (since b = 0)
  a[1] = g(z[1]) = z[1]                 (since g(z) = z)
  â†’ a[1] = W[1] Â· x

Layer 2:
  z[2] = W[2] Â· a[1] = W[2] Â· W[1] Â· x
  a[2] = g(z[2]) = z[2]
  â†’ a[2] = W[2] Â· W[1] Â· x

Layer 3:
  a[3] = W[3] Â· W[2] Â· W[1] Â· x

...continuing this pattern...

Output:
  Å· = W[L] Â· W[L-1] Â· ... Â· W[2] Â· W[1] Â· x
```

So Å· is the **product of all weight matrices** applied to x.

## Case 1: Exploding Gradients (Weights > 1)

### Scenario: Each Weight Matrix Slightly Larger Than Identity

Let's say each weight matrix W[l] is:

```
W[l] = [1.5   0  ]
       [0    1.5 ]
       
= 1.5 Ã— I  (where I is the identity matrix)
```

**Note:** Technically W[L] has different dimensions, but let's focus on the pattern for W[1] through W[L-1].

### What Happens to Å·?

```
Å· = W[L-1] Â· W[L-2] Â· ... Â· W[2] Â· W[1] Â· x

If each W[l] â‰ˆ 1.5 Ã— I:

Å· â‰ˆ (1.5 Ã— I)^(L-1) Â· x
  = 1.5^(L-1) Ã— I^(L-1) Â· x
  = 1.5^(L-1) Ã— x
```

### The Exponential Growth

```
L = 2:   Å· = 1.5^1 Ã— x  = 1.5x
L = 3:   Å· = 1.5^2 Ã— x  = 2.25x
L = 5:   Å· = 1.5^4 Ã— x  = 5.06x
L = 10:  Å· = 1.5^9 Ã— x  = 38.4x
L = 20:  Å· = 1.5^19 Ã— x = 1,477x
L = 50:  Å· = 1.5^49 Ã— x = 2,448,641x   âš ï¸âš ï¸
L = 100: Å· = 1.5^99 Ã— x = 4 Ã— 10^17 x  âš ï¸âš ï¸âš ï¸
L = 150: Å· = 1.5^149 Ã— x = HUGE!!!     âš ï¸âš ï¸âš ï¸
```

**Result:** For a very deep neural network, the value of Å· will **explode**!

It grows **exponentially** as a function of the number of layers L.

### Example with Input x = [1, 1]áµ€

Let's trace the activations through the network:

```
Layer 0 (input):  a[0] = [1.0, 1.0]
Layer 1:          a[1] = [1.5, 1.5]
Layer 2:          a[2] = [2.25, 2.25]
Layer 3:          a[3] = [3.375, 3.375]
Layer 4:          a[4] = [5.06, 5.06]
...
Layer L:          a[L] = [HUGE, HUGE]

Activations EXPLODE! ğŸ’¥
```

## Case 2: Vanishing Gradients (Weights < 1)

### Scenario: Each Weight Matrix Slightly Smaller Than Identity

Now let's say each weight matrix W[l] is:

```
W[l] = [0.5   0  ]
       [0    0.5 ]
       
= 0.5 Ã— I
```

### What Happens to Å·?

```
Å· â‰ˆ (0.5 Ã— I)^(L-1) Â· x
  = 0.5^(L-1) Ã— x
```

### The Exponential Decay

```
L = 2:   Å· = 0.5^1 Ã— x  = 0.5x
L = 3:   Å· = 0.5^2 Ã— x  = 0.25x
L = 5:   Å· = 0.5^4 Ã— x  = 0.0625x
L = 10:  Å· = 0.5^9 Ã— x  = 0.00195x
L = 20:  Å· = 0.5^19 Ã— x = 0.0000019x
L = 50:  Å· = 0.5^49 Ã— x = 1.78 Ã— 10^-15 x  âš ï¸âš ï¸
L = 100: Å· = 0.5^99 Ã— x = 1.58 Ã— 10^-30 x  âš ï¸âš ï¸âš ï¸
L = 150: Å· = 0.5^149 Ã— x â‰ˆ 0                âš ï¸âš ï¸âš ï¸
```

**Result:** The activations **decrease exponentially** as a function of L.

In a very deep network, activations end up **vanishing** to essentially zero!

### Example with Input x = [1, 1]áµ€

```
Layer 0 (input):  a[0] = [1.0, 1.0]
Layer 1:          a[1] = [0.5, 0.5]
Layer 2:          a[2] = [0.25, 0.25]
Layer 3:          a[3] = [0.125, 0.125]
Layer 4:          a[4] = [0.0625, 0.0625]
...
Layer L:          a[L] â‰ˆ [0, 0]

Activations VANISH! ğŸ”»
```

## Visual Summary

### Exploding Activations (W > I)

```
Activation
Magnitude
    â†‘
    â”‚                                    â€¢  â† Layer 150
    â”‚                                   /
    â”‚                               â€¢  /
    â”‚                              /  
 10â¶â”‚                          â€¢  /
    â”‚                         /
    â”‚                     â€¢  /
 10Â³â”‚                    /
    â”‚                â€¢  /
    â”‚               /
  10â”‚           â€¢  /
    â”‚          /
   1â”‚      â€¢  /
    â”‚     /
    â”‚  â€¢ /
    â””â”€â”€â€¢â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Layer
       1  10    50      100    150
       
     Exponential growth! (W = 1.5 Ã— I)
```

### Vanishing Activations (W < I)

```
Activation
Magnitude
    â†‘
   1â”‚  â€¢
    â”‚   \
    â”‚    \  â€¢
0.1 â”‚     \
    â”‚      \
    â”‚       \  â€¢
10â»Â³â”‚        \
    â”‚         \
    â”‚          \  â€¢
10â»â¶â”‚           \
    â”‚            \
    â”‚             \  â€¢
10â»Â¹âµâ”‚             \___â€¢___â€¢___
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Layer
         1  10    50   100   150
         
       Exponential decay! (W = 0.5 Ã— I)
```

## The Key Intuition

### Weights Slightly > Identity â†’ Exploding

```
If W â‰ˆ [1.1  0  ]  (just a bit > 1)
       [0   1.1 ]

Then with very deep network:
  Activations â†’ 1.1^L â†’ EXPLODES as L â†‘
```

### Weights Slightly < Identity â†’ Vanishing

```
If W â‰ˆ [0.9  0  ]  (just a bit < 1)
       [0   0.9 ]

Then with very deep network:
  Activations â†’ 0.9^L â†’ VANISHES as L â†‘
```

## Impact on Gradients

### Similar Problem for Gradients

The same reasoning applies to **derivatives/gradients**:

**During backpropagation:**
- Gradients also get multiplied by weight matrices
- They flow backwards through the network
- Same exponential compounding effect!

```
Exploding Gradients:  âˆ‚J/âˆ‚W[1] âˆ W^L â†’ HUGE
Vanishing Gradients:  âˆ‚J/âˆ‚W[1] âˆ W^L â†’ ~0
```

### Why This is a Problem

#### Exploding Gradients

```
Gradient is HUGE:
  â†’ Weight update: W := W - Î± Ã— (HUGE gradient)
  â†’ Weights change drastically
  â†’ Training becomes unstable
  â†’ May diverge (NaN values)
```

#### Vanishing Gradients

```
Gradient is ~0:
  â†’ Weight update: W := W - Î± Ã— (tiny gradient)
  â†’ Weights barely change
  â†’ Learning is extremely slow
  â†’ Early layers don't learn
  â†’ Gradient descent takes tiny steps
```

**If gradients are exponentially smaller than L:**
- Gradient descent will take **tiny little steps**
- It will take a **long time** to learn anything
- Early layers essentially **freeze**

## Modern Deep Networks

### The Scale of the Problem

**Modern neural networks can be very deep:**

- Microsoft recently achieved great results with a **152-layer neural network**
- Many successful architectures have L = 50, 100, 150+ layers

**With such deep networks:**
- If W > I: activations/gradients increase exponentially
- If W < I: activations/gradients decrease exponentially

```
Example: L = 150

If W = 1.1 Ã— I:  Factor = 1.1^150 = 3.8 Ã— 10^6  (explodes!)
If W = 0.9 Ã— I:  Factor = 0.9^150 = 6.5 Ã— 10^-8 (vanishes!)
```

### Historical Context

**For a long time, this problem was a huge barrier to training deep neural networks.**

This is why deep learning took so long to become practical - the vanishing/exploding gradient problem made very deep networks nearly impossible to train!

## Summary

### The Problem

Deep networks suffer from:

| Issue | Cause | Effect |
|-------|-------|--------|
| **Exploding Gradients** | W slightly > I | Activations/gradients grow exponentially with depth |
| **Vanishing Gradients** | W slightly < I | Activations/gradients shrink exponentially with depth |

### Why It's a Problem

**Exploding:**
- ğŸ’¥ Training becomes unstable
- ğŸ’¥ Weights update too dramatically
- ğŸ’¥ May diverge or produce NaN

**Vanishing:**
- ğŸ”» Learning becomes extremely slow
- ğŸ”» Early layers don't learn
- ğŸ”» Gradient descent stuck

### Mathematical Core

```
For L layers:

Å· âˆ W^L

If W > 1:  W^L â†’ âˆ     (exponential growth)
If W < 1:  W^L â†’ 0     (exponential decay)

Same applies to gradients in backprop!
```

## What's Next: The Partial Solution

**There's a partial solution** that doesn't completely solve this problem, but **helps a lot**:

### Careful Choice of Weight Initialization

How you initialize your weights can significantly reduce the vanishing/exploding gradient problem!

Let's explore this in the next video.

---

## Quick Reference

### The Pattern

```
Deep Network Depth:
  L = 10   â†’ Manageable
  L = 50   â†’ Problems start
  L = 100  â†’ Serious issues
  L = 150+ â†’ Critical without proper techniques

Compounding Effect:
  1.1^10  = 2.6      (acceptable)
  1.1^50  = 117      (problematic)
  1.1^100 = 13,781   (severe)
  1.1^150 = 1.6Ã—10â¶  (catastrophic)

  0.9^10  = 0.35     (acceptable)
  0.9^50  = 0.005    (problematic)
  0.9^100 = 0.000027 (severe)
  0.9^150 â‰ˆ 0        (catastrophic)
```

### Activation Trajectory

```
EXPLODING (W = 1.5 Ã— I):
  Layer 1:   a = 1.5
  Layer 10:  a = 58
  Layer 50:  a = 2.4 million    âš ï¸
  Layer 100: a = 4.4 Ã— 10Â¹â·    âš ï¸âš ï¸
  
VANISHING (W = 0.5 Ã— I):
  Layer 1:   a = 0.5
  Layer 10:  a = 0.001
  Layer 50:  a = 1.8 Ã— 10â»Â¹âµ   âš ï¸
  Layer 100: a â‰ˆ 0              âš ï¸âš ï¸
```

### The Core Issue

**Multiplicative compounding over many layers** amplifies small deviations from identity:
- Slightly > 1 â†’ Explosion
- Slightly < 1 â†’ Vanishing
- Need W â‰ˆ I for stable gradients through many layers
