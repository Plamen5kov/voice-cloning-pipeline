{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d635b0dd",
   "metadata": {},
   "source": [
    "# Audio Classification with One Hidden Layer\n",
    "\n",
    "Welcome to your neural network programming assignment! It's time to build your first neural network with a hidden layer. You'll see a big difference between this model and logistic regression.\n",
    "\n",
    "By the end of this assignment, you'll be able to:\n",
    "\n",
    "- Implement a 2-class classification neural network with a single hidden layer\n",
    "- Use units with a non-linear activation function, such as tanh\n",
    "- Compute the cross entropy loss\n",
    "- Implement forward and backward propagation\n",
    "- Apply this to audio classification (male vs female voices)\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Load the Dataset](#2)\n",
    "    - [Exercise 1](#ex-1)\n",
    "- [3 - Neural Network Model](#3)\n",
    "    - [3.1 - Defining the Neural Network Structure](#3-1)\n",
    "        - [Exercise 2 - layer_sizes](#ex-2)\n",
    "    - [3.2 - Initialize the Model's Parameters](#3-2)\n",
    "        - [Exercise 3 - initialize_parameters](#ex-3)\n",
    "    - [3.3 - The Loop](#3-3)\n",
    "        - [Exercise 4 - forward_propagation](#ex-4)\n",
    "    - [3.4 - Compute the Cost](#3-4)\n",
    "        - [Exercise 5 - compute_cost](#ex-5)\n",
    "    - [3.5 - Implement Backpropagation](#3-5)\n",
    "        - [Exercise 6 - backward_propagation](#ex-6)\n",
    "    - [3.6 - Update Parameters](#3-6)\n",
    "        - [Exercise 7 - update_parameters](#ex-7)\n",
    "    - [3.7 - Integration](#3-7)\n",
    "        - [Exercise 8 - nn_model](#ex-8)\n",
    "- [4 - Test the Model](#4)\n",
    "    - [4.1 - Predict](#4-1)\n",
    "        - [Exercise 9 - predict](#ex-9)\n",
    "    - [4.2 - Test the Model on the Audio Dataset](#4-2)\n",
    "- [5 - Tuning Hidden Layer Size](#5)\n",
    "\n",
    "<a name='1'></a>\n",
    "## 1 - Packages\n",
    "\n",
    "First import all the packages that you will need during this assignment.\n",
    "\n",
    "- **numpy** is the fundamental package for scientific computing with Python.\n",
    "- **matplotlib** is a library for plotting graphs in Python.\n",
    "- **librosa** is a library for audio analysis.\n",
    "- **IPython.display.Audio** allows playing audio in the notebook.\n",
    "- **testCases_v2** provides test examples to assess the correctness of your functions.\n",
    "- **public_tests** provides test functions for grading.\n",
    "- **audio_utils** provides audio loading and preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd38b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "from testCases_v2 import *\n",
    "from public_tests import *\n",
    "from audio_utils import load_dataset, sigmoid\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faec764",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Load the Dataset\n",
    "\n",
    "Let's load the audio dataset for male vs female voice classification. The dataset contains mel-spectrograms extracted from 3-second audio clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96bd8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_x, train_y, test_x, test_y, classes = load_dataset()\n",
    "\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Training set: {train_x.shape[1]} examples\")\n",
    "print(f\"Test set: {test_x.shape[1]} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714aef5e",
   "metadata": {},
   "source": [
    "The dataset consists of:\n",
    "- **train_x**: Training features (mel-spectrograms) of shape (n_features, m_train)\n",
    "- **train_y**: Training labels of shape (1, m_train) where 0 = female, 1 = male\n",
    "- **test_x**: Test features of shape (n_features, m_test)\n",
    "- **test_y**: Test labels of shape (1, m_test)\n",
    "- **classes**: List of class names ['female', 'male']\n",
    "\n",
    "Let's visualize some examples from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9799834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few examples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < train_x.shape[1]:\n",
    "        # Reshape back to 2D spectrogram (assuming 128 mel bands)\n",
    "        n_mels = 128\n",
    "        spec = train_x[:, i].reshape(n_mels, -1)\n",
    "        \n",
    "        # Plot spectrogram\n",
    "        img = librosa.display.specshow(spec, ax=ax, x_axis='time', y_axis='mel')\n",
    "        label = 'Male' if train_y[0, i] == 1 else 'Female'\n",
    "        ax.set_title(f'Sample {i}: {label}')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c94a9b",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - Dataset Exploration\n",
    "\n",
    "How many training examples do you have? What is the `shape` of the variables `train_x` and `train_y`?\n",
    "\n",
    "**Hint**: How do you get the shape of a numpy array? [(help)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (≈ 3 lines of code)\n",
    "# shape_X = ...\n",
    "# shape_Y = ...\n",
    "# m = ...\n",
    "# YOUR CODE STARTS HERE\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print('The shape of train_x is: ' + str(shape_X))\n",
    "print('The shape of train_y is: ' + str(shape_Y))\n",
    "print('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b629b7",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Neural Network Model\n",
    "\n",
    "Now you're going to build a neural network with a single hidden layer for audio classification.\n",
    "\n",
    "**The Model Architecture**:\n",
    "\n",
    "<font color='cyan'>\n",
    "Input → Hidden Layer (with tanh activation) → Output Layer (with sigmoid activation)\n",
    "</font>\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$\n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\text{if } a^{[2](i)} > 0.5 \\\\ 0 & \\text{otherwise} \\end{cases}\\tag{5}$$\n",
    "\n",
    "The cost function $J$:\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**General Methodology**:\n",
    "1. Define the neural network structure (# of input units, # of hidden units, etc)\n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Implement forward propagation\n",
    "    - Compute loss\n",
    "    - Implement backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - Defining the Neural Network Structure\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - layer_sizes\n",
    "\n",
    "Define three variables:\n",
    "- n_x: the size of the input layer\n",
    "- n_h: the size of the hidden layer (set this to 4)\n",
    "- n_y: the size of the output layer\n",
    "\n",
    "**Hint**: Use shapes of X and Y to find n_x and n_y. Hard code the hidden layer size to be 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b685999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    #(≈ 3 lines of code)\n",
    "    # n_x = ... \n",
    "    # n_h = ...\n",
    "    # n_y = ... \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727119fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_X, t_Y = layer_sizes_test_case()\n",
    "(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))\n",
    "\n",
    "layer_sizes_test(layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d8179",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "The size of the input layer is: n_x = 5\n",
    "The size of the hidden layer is: n_h = 4\n",
    "The size of the output layer is: n_y = 1\n",
    "All tests passed!\n",
    "```\n",
    "\n",
    "<a name='3-2'></a>\n",
    "### 3.2 - Initialize the Model's Parameters\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - initialize_parameters\n",
    "\n",
    "Implement the function `initialize_parameters()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Initialize the weight matrices with random values using `np.random.randn(a,b) × 0.01`\n",
    "- Initialize the bias vectors as zeros using `np.zeros((a,b))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d016ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "n_x, n_h, n_y = initialize_parameters_test_case()\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "initialize_parameters_test(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b084fc",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "W1 = [[-0.00416758 -0.00056267]\n",
    " [-0.02136196  0.01640271]\n",
    " [-0.01793436 -0.00841747]\n",
    " [ 0.00502881 -0.01245288]]\n",
    "b1 = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\n",
    "b2 = [[0.]]\n",
    "All tests passed!\n",
    "```\n",
    "\n",
    "<a name='3-3'></a>\n",
    "### 3.3 - The Loop\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - forward_propagation\n",
    "\n",
    "Implement `forward_propagation()` using these equations:\n",
    "\n",
    "$$Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}$$\n",
    "$$A^{[1]} = \\tanh(Z^{[1]})\\tag{2}$$\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}$$\n",
    "\n",
    "**Instructions**:\n",
    "- Use the `sigmoid()` function (already imported from audio_utils)\n",
    "- Use `np.tanh()` for the tanh activation\n",
    "- Store intermediate values in a cache dictionary for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f29a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    # (≈ 4 lines of code)\n",
    "    # Z1 = ...\n",
    "    # A1 = ...\n",
    "    # Z2 = ...\n",
    "    # A2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_X, parameters = forward_propagation_test_case()\n",
    "A2, cache = forward_propagation(t_X, parameters)\n",
    "print(\"A2 = \" + str(A2))\n",
    "\n",
    "forward_propagation_test(forward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15fe39",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "A2 = [[0.26541412 0.42229234 0.55544138]]\n",
    "All tests passed!\n",
    "```\n",
    "\n",
    "<a name='3-4'></a>\n",
    "### 3.4 - Compute the Cost\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - compute_cost\n",
    "\n",
    "Implement `compute_cost()` to compute the value of the cost $J$.\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{6}$$\n",
    "\n",
    "**Instructions**:\n",
    "- Use `np.multiply()` and `np.sum()` or `np.dot()` to compute the cost\n",
    "- Use `np.squeeze()` to ensure the cost is a scalar (single number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81964c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    # (≈ 2 lines of code)\n",
    "    # logprobs = ...\n",
    "    # cost = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = float(np.squeeze(cost))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02160ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2, t_Y = compute_cost_test_case()\n",
    "cost = compute_cost(A2, t_Y)\n",
    "print(\"cost = \" + str(cost))\n",
    "\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4dc35",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "cost = 0.6930587610394646\n",
    "All tests passed!\n",
    "```\n",
    "\n",
    "<a name='3-5'></a>\n",
    "### 3.5 - Implement Backpropagation\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercise 6 - backward_propagation\n",
    "\n",
    "Implement the function `backward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "Backpropagation computes the gradients of the cost with respect to parameters. Use these formulas:\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "$$db^{[2]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[2](i)}$$\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} * (1 - A^{[1]2})$$\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^{T}$$\n",
    "$$db^{[1]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[1](i)}$$\n",
    "\n",
    "**Note**: × denotes element-wise multiplication. For tanh: $g'(z) = 1 - a^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ae79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    X -- input data of shape (n_x, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Retrieve W1 and W2 from dictionary \"parameters\"\n",
    "    #(≈ 2 lines of code)\n",
    "    # W1 = ...\n",
    "    # W2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "        \n",
    "    # Retrieve A1 and A2 from dictionary \"cache\"\n",
    "    #(≈ 2 lines of code)\n",
    "    # A1 = ...\n",
    "    # A2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2\n",
    "    #(≈ 6 lines of code)\n",
    "    # dZ2 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    # dZ1 = ...\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, cache, t_X, t_Y = backward_propagation_test_case()\n",
    "\n",
    "grads = backward_propagation(parameters, cache, t_X, t_Y)\n",
    "print(\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print(\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print(\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print(\"db2 = \"+ str(grads[\"db2\"]))\n",
    "\n",
    "backward_propagation_test(backward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0232179",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "All tests passed!\n",
    "```\n",
    "\n",
    "<a name='3-6'></a>\n",
    "### 3.6 - Update Parameters\n",
    "\n",
    "<a name='ex-7'></a>\n",
    "### Exercise 7 - update_parameters\n",
    "\n",
    "Implement the update rule using gradient descent:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
    "\n",
    "**Hint**: Use `copy.deepcopy()` for W1 and W2 to avoid modifying the original parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve a copy of each parameter from dictionary \"parameters\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Retrieve each gradient from dictionary \"grads\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "update_parameters_test(update_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d65b79",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "All tests passed!\n",
    "```\n",
    "\n",
    "<a name='3-7'></a>\n",
    "### 3.7 - Integration\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Exercise 8 - nn_model\n",
    "\n",
    "Build your neural network model by integrating all previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655286b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations=10000, learning_rate=0.01, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (n_x, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        #(≈ 4 lines of code)\n",
    "        # A2, cache = ...\n",
    "        # cost = ...\n",
    "        # grads = ...\n",
    "        # parameters = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_test(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd24d1",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Cost after iteration 0: ...\n",
    "Cost after iteration 1000: ...\n",
    "...\n",
    "All tests passed!\n",
    "```\n",
    "\n",
    "<a name='4'></a>\n",
    "## 4 - Test the Model\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Predict\n",
    "\n",
    "<a name='ex-9'></a>\n",
    "### Exercise 9 - predict\n",
    "\n",
    "Use forward propagation to predict results.\n",
    "\n",
    "$$predictions = \\mathbb{1}_{\\{activation > 0.5\\}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- vector of predictions (0 for female / 1 for male)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation\n",
    "    #(≈ 2 lines of code)\n",
    "    # A2, cache = ...\n",
    "    # predictions = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, t_X = predict_test_case()\n",
    "\n",
    "predictions = predict(parameters, t_X)\n",
    "print(\"Predictions: \" + str(predictions))\n",
    "\n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6a8d2",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Predictions: [[ True False  True]]\n",
    "All tests passed!\n",
    "```\n",
    "\n",
    "<a name='4-2'></a>\n",
    "### 4.2 - Test the Model on the Audio Dataset\n",
    "\n",
    "Time to train the model on real audio data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a207b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with a 4-unit hidden layer\n",
    "print(\"Training neural network with 4 hidden units...\")\n",
    "parameters = nn_model(train_x, train_y, n_h=4, num_iterations=3000, learning_rate=0.01, print_cost=True)\n",
    "\n",
    "# Make predictions\n",
    "predictions_train = predict(parameters, train_x)\n",
    "predictions_test = predict(parameters, test_x)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = ((np.dot(train_y, predictions_train.T) + np.dot(1 - train_y, 1 - predictions_train.T)) / float(train_y.size) * 100).item()\n",
    "test_accuracy = ((np.dot(test_y, predictions_test.T) + np.dot(1 - test_y, 1 - predictions_test.T)) / float(test_y.size) * 100).item()\n",
    "\n",
    "print(f'\\nTraining Accuracy: {train_accuracy:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b187b8d",
   "metadata": {},
   "source": [
    "Great job! Your neural network can now classify male and female voices with high accuracy! \n",
    "\n",
    "The model has learned patterns in the audio spectrograms that distinguish between male and female voices, such as:\n",
    "- **Pitch differences**: Female voices typically have higher fundamental frequencies\n",
    "- **Formant patterns**: Different resonance characteristics\n",
    "- **Spectral shape**: Overall energy distribution across frequencies\n",
    "\n",
    "<a name='5'></a>\n",
    "## 5 - Tuning Hidden Layer Size\n",
    "\n",
    "Let's experiment with different hidden layer sizes to see how it affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different hidden layer sizes\n",
    "plt.figure(figsize=(12, 8))\n",
    "hidden_layer_sizes = [1, 2, 3]\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for n_h in hidden_layer_sizes:\n",
    "    print(f\"\\nTraining with {n_h} hidden units...\")\n",
    "    parameters = nn_model(train_x, train_y, n_h, num_iterations=3000, print_cost=False)\n",
    "    \n",
    "    predictions_train = predict(parameters, train_x)\n",
    "    predictions_test = predict(parameters, test_x)\n",
    "    \n",
    "    train_acc = ((np.dot(train_y, predictions_train.T) + np.dot(1 - train_y, 1 - predictions_train.T)) / float(train_y.size) * 100).item()\n",
    "    test_acc = ((np.dot(test_y, predictions_test.T) + np.dot(1 - test_y, 1 - predictions_test.T)) / float(test_y.size) * 100).item()\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"  Train accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"  Test accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Plot results\n",
    "plt.plot(hidden_layer_sizes, train_accuracies, 'o-', label='Train Accuracy', linewidth=2)\n",
    "plt.plot(hidden_layer_sizes, test_accuracies, 's-', label='Test Accuracy', linewidth=2)\n",
    "plt.xlabel('Hidden Layer Size', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Model Performance vs Hidden Layer Size', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e43539",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "- **Small hidden layers** (1-2 units): May underfit - not enough capacity to learn complex patterns\n",
    "- **Medium hidden layers** (3-5 units): Often provide good balance between performance and generalization\n",
    "- **Large hidden layers** (10+ units): May overfit the training data, especially with limited training examples\n",
    "\n",
    "The optimal hidden layer size depends on:\n",
    "- Complexity of the task\n",
    "- Amount of training data\n",
    "- Regularization techniques (covered in later modules)\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "✅ Built a complete 2-class neural network with a hidden layer  \n",
    "✅ Implemented forward and backward propagation from scratch  \n",
    "✅ Applied it to real audio classification  \n",
    "✅ Analyzed the impact of hidden layer size  \n",
    "\n",
    "You've taken a major step in deep learning! This neural network architecture is the foundation for much larger and more complex networks.\n",
    "\n",
    "**Next steps to explore**:\n",
    "- Try different activation functions (ReLU, Leaky ReLU)\n",
    "- Experiment with different learning rates\n",
    "- Add more hidden layers (deep networks)\n",
    "- Try other audio classification tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
