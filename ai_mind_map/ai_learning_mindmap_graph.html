<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Learning Path: Voice Cloning Pipeline - Graph View</title>
  <style>
    * {
      margin: 0;
      padding: 0;
    }
    body {
      font-family: 'Inter', 'Segoe UI', Roboto, -apple-system, sans-serif;
      overflow: hidden;
    }
    #graph {
      width: 100vw;
      height: 100vh;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }
    .title {
      position: absolute;
      top: 20px;
      left: 20px;
      background: rgba(255, 255, 255, 0.95);
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 4px 15px rgba(0,0,0,0.3);
      z-index: 1000;
      max-width: 400px;
    }
    .title h1 {
      font-size: 18px;
      margin-bottom: 8px;
      color: #333;
      font-weight: 400;
    }
    .title p {
      font-size: 12px;
      color: #666;
      line-height: 1.4;
      font-weight: 300;
    }
    .controls {
      position: absolute;
      bottom: 20px;
      right: 20px;
      background: rgba(255, 255, 255, 0.95);
      padding: 15px;
      border-radius: 10px;
      box-shadow: 0 4px 15px rgba(0,0,0,0.3);
      z-index: 1000;
    }
    .controls button {
      margin: 5px;
      padding: 8px 15px;
      border: none;
      border-radius: 5px;
      background: #667eea;
      color: white;
      cursor: pointer;
      font-size: 12px;
      transition: background 0.3s;
    }
    .controls button:hover {
      background: #764ba2;
    }
    .node {
      cursor: pointer;
      stroke: #fff;
      stroke-width: 2px;
    }
    .node text {
      font-size: 11px;
      fill: #333;
      font-weight: 300;
      pointer-events: none;
      text-shadow: 0px 0px 4px white, 0px 0px 4px white, 0px 0px 2px white;
    }
    .link {
      stroke: rgba(255, 255, 255, 0.4);
      stroke-width: 1.5px;
    }
    .tooltip {
      position: absolute;
      background: rgba(0, 0, 0, 0.9);
      color: white;
      padding: 10px;
      border-radius: 5px;
      font-size: 12px;
      pointer-events: none;
      opacity: 0;
      transition: opacity 0.3s;
      max-width: 300px;
      z-index: 2000;
    }
    .info-panel {
      position: absolute;
      top: 20px;
      right: 20px;
      background: rgba(255, 255, 255, 0.98);
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 4px 20px rgba(0,0,0,0.3);
      z-index: 1000;
      max-width: 350px;
      max-height: 80vh;
      overflow-y: auto;
      display: none;
    }
    .info-panel.visible {
      display: block;
    }
    .info-panel h2 {
      font-size: 18px;
      margin-bottom: 10px;
      color: #333;
      font-weight: 400;
    }
    .info-panel p {
      font-size: 13px;
      color: #555;
      line-height: 1.6;
      margin-bottom: 10px;
      font-weight: 300;
    }
    .info-panel .close-btn {
      position: absolute;
      top: 10px;
      right: 10px;
      background: none;
      border: none;
      font-size: 24px;
      color: #999;
      cursor: pointer;
      padding: 0;
      width: 30px;
      height: 30px;
      line-height: 30px;
      text-align: center;
    }
    .info-panel .close-btn:hover {
      color: #333;
    }
    .info-panel .links {
      margin-top: 15px;
      padding-top: 15px;
      border-top: 1px solid #ddd;
    }
    .info-panel .links a {
      display: block;
      padding: 8px 12px;
      margin: 5px 0;
      background: #667eea;
      color: white;
      text-decoration: none;
      border-radius: 5px;
      font-size: 12px;
      font-weight: 300;
      transition: background 0.3s;
    }
    .info-panel .links a:hover {
      background: #764ba2;
    }
    .info-panel ul {
      margin: 10px 0;
      padding-left: 20px;
    }
    .info-panel li {
      font-size: 12px;
      color: #555;
      line-height: 1.8;
      font-weight: 300;
    }
  </style>
</head>
<body>
  <div class="title">
    <h1>ðŸŽ“ AI Learning Path: Voice Cloning Pipeline</h1>
    <p>Interactive network graph - Drag nodes to rearrange. Click any node to view details and resources.</p>
  </div>
  
  <div class="controls">
    <button onclick="resetSimulation()">Reset Layout</button>
    <button onclick="centerView()">Center View</button>
  </div>
  
  <div class="info-panel" id="infoPanel">
    <button class="close-btn" onclick="closeInfoPanel()">Ã—</button>
    <div id="infoPanelContent"></div>
  </div>
  
  <div class="tooltip" id="tooltip"></div>
  <svg id="graph"></svg>
  
  <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
  
  <script>
    // Detailed information for each node
    const nodeInfo = {
      0: {
        title: "Voice Cloning Pipeline",
        description: "Complete end-to-end learning path for building production-ready voice cloning systems.",
        details: ["Combines deep learning, audio processing, and NLP", "Covers foundation to deployment", "Hands-on labs and projects"],
        links: [
          { text: "Main README", url: "README.md" },
          { text: "Mind Map Overview", url: "ai_mind_map/README.md" }
        ]
      },
      1: {
        title: "Environment Setup",
        description: "Set up your development environment with Python, ML libraries, and GPU acceleration.",
        details: ["Virtual environments (venv, conda)", "CUDA and GPU drivers", "PyTorch, TensorFlow installation", "Jupyter notebooks"],
        links: [
          { text: "Setup Guide", url: "00_env_setup/README.md" },
          { text: "Learning Guide", url: "00_env_setup/LEARNING_GUIDE.md" }
        ]
      },
      2: {
        title: "Python Programming",
        description: "Master Python fundamentals needed for AI/ML development.",
        details: ["File I/O operations", "Audio file conversion", "Text processing", "Scripting automation"],
        links: [
          { text: "Python Module README", url: "01_python_programming/README.md" },
          { text: "Learning Guide", url: "01_python_programming/LEARNING_GUIDE.md" }
        ]
      },
      3: {
        title: "Deep Learning Fundamentals",
        description: "Learn neural networks, PyTorch, training processes, and optimization.",
        details: ["PyTorch tensors and autograd", "Neural network architectures", "Training loops", "Hyperparameter tuning", "8+ hands-on labs"],
        links: [
          { text: "Deep Learning Labs", url: "02_deep_learning_basics/" },
          { text: "Logistic Regression Lab", url: "02_deep_learning_basics/01_lab_logistic_regression/README.md" }
        ]
      },
      4: {
        title: "Natural Language Processing",
        description: "Text processing fundamentals for preparing text input for TTS systems.",
        details: ["Tokenization techniques", "Text normalization", "Named entity recognition", "Sentiment analysis"],
        links: [
          { text: "NLP README", url: "05_nlp/README.md" },
          { text: "Learning Guide", url: "05_nlp/LEARNING_GUIDE.md" }
        ]
      },
      5: {
        title: "Transformers & Hugging Face",
        description: "Master transformer architecture and the Hugging Face ecosystem.",
        details: ["Self-attention mechanism", "BERT, GPT architectures", "Hugging Face Model Hub", "Fine-tuning workflows"],
        links: [
          { text: "Transformers README", url: "06_hf_transformers/README.md" },
          { text: "Learning Guide", url: "06_hf_transformers/LEARNING_GUIDE.md" }
        ]
      },
      6: {
        title: "Speech & Audio Processing",
        description: "Learn audio fundamentals, feature extraction, and voice analysis.",
        details: ["Waveforms and spectrograms", "Mel-spectrograms and MFCCs", "Audio normalization", "Voice activity detection"],
        links: [
          { text: "Audio Processing README", url: "04_speech_audio_processing/README.md" },
          { text: "Learning Guide", url: "04_speech_audio_processing/LEARNING_GUIDE.md" }
        ]
      },
      7: {
        title: "Text-to-Speech Systems",
        description: "Explore TTS architectures and voice cloning techniques.",
        details: ["Tacotron 2, FastSpeech", "XTTS and Bark models", "Voice cloning methods", "Few-shot learning"],
        links: [
          { text: "TTS README", url: "03_tts_systems/README.md" },
          { text: "Learning Guide", url: "03_tts_systems/LEARNING_GUIDE.md" }
        ]
      },
      8: {
        title: "Generative AI",
        description: "Learn generative models including GANs, VAEs, and transformer-based generation.",
        details: ["GANs and VAEs", "GPT-based text generation", "Prompt engineering", "Creative applications"],
        links: [
          { text: "Generative AI README", url: "09_generative_ai/README.md" },
          { text: "Learning Guide", url: "09_generative_ai/LEARNING_GUIDE.md" }
        ]
      },
      9: {
        title: "Data Preparation",
        description: "Prepare high-quality datasets for voice cloning training.",
        details: ["Audio recording best practices", "Dataset organization", "Quality control", "Preprocessing pipelines"],
        links: [
          { text: "Data Prep README", url: "07_data_preparation/README.md" },
          { text: "Learning Guide", url: "07_data_preparation/LEARNING_GUIDE.md" }
        ]
      },
      10: {
        title: "Model Training & Fine-tuning",
        description: "Train and fine-tune voice cloning models effectively.",
        details: ["Training loops", "Loss monitoring", "Checkpoint management", "Multi-GPU training"],
        links: [
          { text: "Training README", url: "08_model_training_finetuning/README.md" },
          { text: "Learning Guide", url: "08_model_training_finetuning/LEARNING_GUIDE.md" }
        ]
      },
      11: {
        title: "MLOps & Deployment",
        description: "Deploy and monitor ML models in production.",
        details: ["Model serving", "API design", "Monitoring and logging", "CI/CD pipelines"],
        links: [
          { text: "MLOps README", url: "10_mlops/README.md" },
          { text: "Learning Guide", url: "10_mlops/LEARNING_GUIDE.md" }
        ]
      },
      12: {
        title: "Cloud Platforms",
        description: "Deploy on AWS, GCP, and Azure with containers.",
        details: ["Docker containerization", "AWS/GCP/Azure services", "Scalability patterns", "Cost optimization"],
        links: [
          { text: "Cloud README", url: "11_cloud_platforms/README.md" },
          { text: "Learning Guide", url: "11_cloud_platforms/LEARNING_GUIDE.md" }
        ]
      },
      // Level 2 nodes
      13: { title: "Virtual Environments", description: "Manage Python dependencies with virtual environments.", details: ["venv, conda, virtualenv", "Dependency isolation", "requirements.txt"], links: [] },
      14: { title: "GPU/CUDA Setup", description: "Configure GPU acceleration for deep learning.", details: ["NVIDIA drivers", "CUDA toolkit", "cuDNN libraries"], links: [] },
      15: { title: "ML Libraries", description: "Install essential ML frameworks.", details: ["PyTorch with CUDA", "TensorFlow", "scikit-learn"], links: [] },
      16: { title: "File I/O", description: "Python file operations for data handling.", details: ["Reading/writing files", "CSV and JSON parsing", "Path management"], links: [] },
      17: { title: "Audio Handling", description: "Process audio files in Python.", details: ["M4A to WAV conversion", "Batch processing", "Format handling"], links: [{ text: "Audio Converter", url: "01_python_programming/convert_m4a_to_wav.py" }] },
      18: { title: "Text Processing", description: "Text manipulation and analysis.", details: ["String operations", "Regular expressions", "Word counting"], links: [{ text: "Text Processing Script", url: "01_python_programming/count_words_sentences.py" }] },
      19: { title: "PyTorch", description: "PyTorch fundamentals for deep learning.", details: ["Tensor operations", "Autograd", "Device management"], links: [{ text: "Hello PyTorch", url: "02_deep_learning_basics/01_hello_pytorch.py" }] },
      20: { title: "Neural Networks", description: "Build and understand neural networks.", details: ["Layers and activations", "Forward/backward pass", "Network architectures"], links: [] },
      21: { title: "Training Process", description: "Train neural networks effectively.", details: ["Loss functions", "Optimizers", "Training loops"], links: [{ text: "MNIST Training", url: "02_deep_learning_basics/03_train_mnist.py" }] },
      22: { title: "Hyperparameters", description: "Tune model hyperparameters.", details: ["Learning rate", "Batch size", "Regularization"], links: [] },
      23: { title: "Feature Extraction", description: "Extract features from audio signals.", details: ["Spectrograms", "Mel-spectrograms", "MFCCs"], links: [{ text: "Feature Extraction Notebook", url: "01_python_programming/notebooks/01_feature_extraction_and_bark_inference.ipynb" }] },
      24: { title: "Audio Processing", description: "Preprocess and enhance audio.", details: ["Normalization", "Resampling", "Noise reduction"], links: [] },
      25: { title: "Voice Analysis", description: "Analyze voice characteristics.", details: ["Pitch detection", "Speaker diarization", "VAD"], links: [] },
      26: { title: "TTS Pipeline", description: "End-to-end TTS system architecture.", details: ["Text analysis", "Phonetic conversion", "Audio synthesis"], links: [] },
      27: { title: "Voice Cloning", description: "Clone voices with few samples.", details: ["Speaker embeddings", "Few-shot learning", "Voice similarity"], links: [{ text: "Voice Cloning Experiment", url: "01_python_programming/notebooks/03_bark_voice_cloning_experiment.ipynb" }] },
      28: { title: "XTTS/Bark", description: "Modern TTS models for voice cloning.", details: ["XTTS by Coqui", "Bark by Suno", "Zero-shot synthesis"], links: [{ text: "XTTS Demo", url: "03_tts_systems/tts_xtts_my_voice_demo.py" }] },
      29: { title: "Tokenization", description: "Break text into tokens.", details: ["Word, subword, character", "BPE and WordPiece", "Tokenizer training"], links: [] },
      30: { title: "Text Analysis", description: "Analyze and preprocess text.", details: ["Sentence segmentation", "Normalization", "Language detection"], links: [] },
      31: { title: "Attention Mechanism", description: "Self-attention in transformers.", details: ["Multi-head attention", "Positional encoding", "Attention weights"], links: [] },
      32: { title: "Hugging Face", description: "Hugging Face ecosystem.", details: ["Model Hub", "Pipeline API", "Datasets library"], links: [] },
      33: { title: "Fine-tuning", description: "Fine-tune pretrained models.", details: ["Transfer learning", "Dataset preparation", "Hyperparameter config"], links: [] },
      34: { title: "Dataset Creation", description: "Build custom voice datasets.", details: ["Recording guidelines", "Metadata format", "File organization"], links: [] },
      35: { title: "Audio Preprocessing", description: "Prepare audio for training.", details: ["Trimming silence", "Normalization", "Quality validation"], links: [] },
      36: { title: "Quality Control", description: "Ensure data quality.", details: ["Audio-text alignment", "Outlier detection", "Consistency checks"], links: [] },
      37: { title: "Training Setup", description: "Configure training environment.", details: ["GPU configuration", "Memory management", "Distributed training"], links: [] },
      38: { title: "Monitoring", description: "Track training metrics.", details: ["Loss tracking", "TensorBoard", "Early stopping"], links: [] },
      39: { title: "Checkpointing", description: "Save and resume training.", details: ["Model checkpoints", "Best model saving", "Resume training"], links: [] },
      40: { title: "Deployment", description: "Deploy models to production.", details: ["Model serving", "Load balancing", "Version control"], links: [] },
      41: { title: "API Design", description: "Build APIs for ML models.", details: ["REST APIs", "FastAPI/Flask", "Input validation"], links: [] },
      42: { title: "Monitoring", description: "Monitor production systems.", details: ["Performance metrics", "Error tracking", "Alerting"], links: [] },
      43: { title: "AWS/GCP", description: "Cloud platform services.", details: ["EC2/Compute Engine", "S3/Cloud Storage", "ML services"], links: [] },
      44: { title: "Docker", description: "Containerize ML applications.", details: ["Dockerfile creation", "Image optimization", "Container orchestration"], links: [] }
    };

    // Data structure for the learning path
    const data = {
      nodes: [
        // Central node
        { id: 0, name: "Voice Cloning Pipeline", level: 0, category: "core" },
        
        // Foundation Layer (Level 1)
        { id: 1, name: "Environment Setup", level: 1, category: "foundation" },
        { id: 2, name: "Python Programming", level: 1, category: "foundation" },
        
        // Core ML (Level 1)
        { id: 3, name: "Deep Learning", level: 1, category: "ml" },
        
        // Domain Specific (Level 1)
        { id: 4, name: "NLP", level: 1, category: "domain" },
        { id: 5, name: "Transformers", level: 1, category: "domain" },
        { id: 6, name: "Speech/Audio", level: 1, category: "domain" },
        { id: 7, name: "TTS Systems", level: 1, category: "domain" },
        { id: 8, name: "Generative AI", level: 1, category: "domain" },
        
        // Data & Training (Level 1)
        { id: 9, name: "Data Preparation", level: 1, category: "data" },
        { id: 10, name: "Model Training", level: 1, category: "data" },
        
        // Production (Level 1)
        { id: 11, name: "MLOps", level: 1, category: "production" },
        { id: 12, name: "Cloud Platforms", level: 1, category: "production" },
        
        // Level 2 - Environment Setup details
        { id: 13, name: "Virtual Environments", level: 2, category: "foundation" },
        { id: 14, name: "GPU/CUDA Setup", level: 2, category: "foundation" },
        { id: 15, name: "ML Libraries", level: 2, category: "foundation" },
        
        // Level 2 - Python Programming details
        { id: 16, name: "File I/O", level: 2, category: "foundation" },
        { id: 17, name: "Audio Handling", level: 2, category: "foundation" },
        { id: 18, name: "Text Processing", level: 2, category: "foundation" },
        
        // Level 2 - Deep Learning details
        { id: 19, name: "PyTorch", level: 2, category: "ml" },
        { id: 20, name: "Neural Networks", level: 2, category: "ml" },
        { id: 21, name: "Training Process", level: 2, category: "ml" },
        { id: 22, name: "Hyperparameters", level: 2, category: "ml" },
        
        // Level 2 - Speech/Audio details
        { id: 23, name: "Feature Extraction", level: 2, category: "domain" },
        { id: 24, name: "Audio Processing", level: 2, category: "domain" },
        { id: 25, name: "Voice Analysis", level: 2, category: "domain" },
        
        // Level 2 - TTS details
        { id: 26, name: "TTS Pipeline", level: 2, category: "domain" },
        { id: 27, name: "Voice Cloning", level: 2, category: "domain" },
        { id: 28, name: "XTTS/Bark", level: 2, category: "domain" },
        
        // Level 2 - NLP details
        { id: 29, name: "Tokenization", level: 2, category: "domain" },
        { id: 30, name: "Text Analysis", level: 2, category: "domain" },
        
        // Level 2 - Transformers details
        { id: 31, name: "Attention", level: 2, category: "domain" },
        { id: 32, name: "Hugging Face", level: 2, category: "domain" },
        { id: 33, name: "Fine-tuning", level: 2, category: "domain" },
        
        // Level 2 - Data Preparation details
        { id: 34, name: "Dataset Creation", level: 2, category: "data" },
        { id: 35, name: "Audio Preprocessing", level: 2, category: "data" },
        { id: 36, name: "Quality Control", level: 2, category: "data" },
        
        // Level 2 - Model Training details
        { id: 37, name: "Training Setup", level: 2, category: "data" },
        { id: 38, name: "Monitoring", level: 2, category: "data" },
        { id: 39, name: "Checkpointing", level: 2, category: "data" },
        
        // Level 2 - MLOps details
        { id: 40, name: "Deployment", level: 2, category: "production" },
        { id: 41, name: "API Design", level: 2, category: "production" },
        { id: 42, name: "Monitoring", level: 2, category: "production" },
        
        // Level 2 - Cloud details
        { id: 43, name: "AWS/GCP", level: 2, category: "production" },
        { id: 44, name: "Docker", level: 2, category: "production" },
      ],
      links: [
        // Foundation connections to core
        { source: 0, target: 1 },
        { source: 0, target: 2 },
        
        // ML connections to core
        { source: 0, target: 3 },
        
        // Domain specific connections to core
        { source: 0, target: 4 },
        { source: 0, target: 5 },
        { source: 0, target: 6 },
        { source: 0, target: 7 },
        { source: 0, target: 8 },
        
        // Data connections to core
        { source: 0, target: 9 },
        { source: 0, target: 10 },
        
        // Production connections to core
        { source: 0, target: 11 },
        { source: 0, target: 12 },
        
        // Environment Setup details
        { source: 1, target: 13 },
        { source: 1, target: 14 },
        { source: 1, target: 15 },
        
        // Python Programming details
        { source: 2, target: 16 },
        { source: 2, target: 17 },
        { source: 2, target: 18 },
        
        // Deep Learning details
        { source: 3, target: 19 },
        { source: 3, target: 20 },
        { source: 3, target: 21 },
        { source: 3, target: 22 },
        
        // Speech/Audio details
        { source: 6, target: 23 },
        { source: 6, target: 24 },
        { source: 6, target: 25 },
        
        // TTS details
        { source: 7, target: 26 },
        { source: 7, target: 27 },
        { source: 7, target: 28 },
        
        // NLP details
        { source: 4, target: 29 },
        { source: 4, target: 30 },
        
        // Transformers details
        { source: 5, target: 31 },
        { source: 5, target: 32 },
        { source: 5, target: 33 },
        
        // Data Preparation details
        { source: 9, target: 34 },
        { source: 9, target: 35 },
        { source: 9, target: 36 },
        
        // Model Training details
        { source: 10, target: 37 },
        { source: 10, target: 38 },
        { source: 10, target: 39 },
        
        // MLOps details
        { source: 11, target: 40 },
        { source: 11, target: 41 },
        { source: 11, target: 42 },
        
        // Cloud details
        { source: 12, target: 43 },
        { source: 12, target: 44 },
        
        // Cross-domain connections (showing dependencies)
        { source: 2, target: 3 },  // Python -> DL
        { source: 3, target: 5 },  // DL -> Transformers
        { source: 3, target: 8 },  // DL -> Generative AI
        { source: 4, target: 7 },  // NLP -> TTS
        { source: 5, target: 7 },  // Transformers -> TTS
        { source: 6, target: 7 },  // Speech/Audio -> TTS
        { source: 7, target: 9 },  // TTS -> Data Prep
        { source: 9, target: 10 }, // Data Prep -> Training
        { source: 10, target: 11 }, // Training -> MLOps
        { source: 11, target: 12 }, // MLOps -> Cloud
        { source: 17, target: 6 },  // Audio Handling -> Speech/Audio
        { source: 19, target: 10 }, // PyTorch -> Training
      ]
    };

    // Color scheme by category
    const colors = {
      core: '#FF6B6B',
      foundation: '#4ECDC4',
      ml: '#45B7D1',
      domain: '#96CEB4',
      data: '#FFEAA7',
      production: '#DDA15E'
    };

    // Setup SVG
    const width = window.innerWidth;
    const height = window.innerHeight;
    
    const svg = d3.select('#graph')
      .attr('width', width)
      .attr('height', height);
    
    const g = svg.append('g');
    
    // Add zoom behavior
    const zoom = d3.zoom()
      .scaleExtent([0.1, 4])
      .on('zoom', (event) => {
        g.attr('transform', event.transform);
      });
    
    svg.call(zoom);
    
    // Close info panel when clicking on background
    svg.on('click', () => {
      closeInfoPanel();
    });

    // Create simulation
    const simulation = d3.forceSimulation(data.nodes)
      .force('link', d3.forceLink(data.links).id(d => d.id).distance(d => {
        // Shorter distances for level 2 nodes
        return d.target.level === 2 ? 60 : 120;
      }))
      .force('charge', d3.forceManyBody().strength(d => {
        // Core node has stronger repulsion
        if (d.level === 0) return -1500;
        if (d.level === 1) return -800;
        return -400;
      }))
      .force('center', d3.forceCenter(width / 2, height / 2))
      .force('collision', d3.forceCollide().radius(40));

    // Create links
    const link = g.append('g')
      .selectAll('line')
      .data(data.links)
      .join('line')
      .attr('class', 'link');

    // Create nodes
    const node = g.append('g')
      .selectAll('g')
      .data(data.nodes)
      .join('g')
      .attr('class', 'node')
      .call(d3.drag()
        .on('start', dragstarted)
        .on('drag', dragged)
        .on('end', dragended));

    // Add circles to nodes
    node.append('circle')
      .attr('r', d => {
        if (d.level === 0) return 30;
        if (d.level === 1) return 20;
        return 12;
      })
      .attr('fill', d => colors[d.category])
      .on('mouseover', showTooltip)
      .on('mouseout', hideTooltip)
      .on('click', (event, d) => {
        event.stopPropagation();
        showInfoPanel(d);
      });

    // Add text to nodes
    node.append('text')
      .attr('dy', d => d.level === 0 ? 45 : d.level === 1 ? 30 : 22)
      .attr('text-anchor', 'middle')
      .text(d => d.name)
      .style('font-size', d => d.level === 0 ? '12px' : d.level === 1 ? '10px' : '9px')
      .style('font-weight', d => d.level === 0 ? '400' : '300');

    // Update positions on tick
    simulation.on('tick', () => {
      link
        .attr('x1', d => d.source.x)
        .attr('y1', d => d.source.y)
        .attr('x2', d => d.target.x)
        .attr('y2', d => d.target.y);

      node.attr('transform', d => `translate(${d.x},${d.y})`);
    });

    // Drag functions
    function dragstarted(event, d) {
      if (!event.active) simulation.alphaTarget(0.3).restart();
      d.fx = d.x;
      d.fy = d.y;
    }

    function dragged(event, d) {
      d.fx = event.x;
      d.fy = event.y;
    }

    function dragended(event, d) {
      if (!event.active) simulation.alphaTarget(0);
      d.fx = null;
      d.fy = null;
    }

    // Tooltip functions
    function showTooltip(event, d) {
      const tooltip = document.getElementById('tooltip');
      tooltip.style.opacity = 1;
      tooltip.style.left = (event.pageX + 10) + 'px';
      tooltip.style.top = (event.pageY + 10) + 'px';
      tooltip.innerHTML = `<strong>${d.name}</strong><br>Click for details`;
    }

    function hideTooltip() {
      document.getElementById('tooltip').style.opacity = 0;
    }

    // Info panel functions
    function showInfoPanel(d) {
      const panel = document.getElementById('infoPanel');
      const content = document.getElementById('infoPanelContent');
      const info = nodeInfo[d.id];
      
      if (!info) return;
      
      let html = `<h2>${info.title}</h2>`;
      html += `<p>${info.description}</p>`;
      
      if (info.details && info.details.length > 0) {
        html += `<ul>`;
        info.details.forEach(detail => {
          html += `<li>${detail}</li>`;
        });
        html += `</ul>`;
      }
      
      if (info.links && info.links.length > 0) {
        html += `<div class="links">`;
        html += `<p style="font-weight: 400; margin-bottom: 8px;">ðŸ“š Resources:</p>`;
        info.links.forEach(link => {
          html += `<a href="${link.url}" target="_blank">${link.text}</a>`;
        });
        html += `</div>`;
      }
      
      content.innerHTML = html;
      panel.classList.add('visible');
    }

    function closeInfoPanel() {
      document.getElementById('infoPanel').classList.remove('visible');
    }

    // Focus on a node (zoom to it)
    function focusNode(event, d) {
      const scale = 1.5;
      const x = -d.x * scale + width / 2;
      const y = -d.y * scale + height / 2;
      
      svg.transition()
        .duration(750)
        .call(zoom.transform, d3.zoomIdentity.translate(x, y).scale(scale));
    }

    // Utility functions
    function resetSimulation() {
      simulation.alpha(1).restart();
    }

    function centerView() {
      svg.transition()
        .duration(750)
        .call(zoom.transform, d3.zoomIdentity);
    }

    // Initial zoom to fit
    setTimeout(() => {
      svg.call(zoom.transform, d3.zoomIdentity.scale(0.7).translate(width * 0.2, height * 0.2));
    }, 100);
  </script>
</body>
</html>
